«Санкт-Петербургский государственный электротехнический университет 
«ЛЭТИ» им. В.И.Ульянова (Ленина)»
(СПбГЭТУ «ЛЭТИ»)

Направление	09.04.02 - Информационные системы и технологии
Программа 	Управление IT проектами и продуктами
Факультет	КТИ
Кафедра	АПУ
К защите допустить	
Зав. кафедрой		Шестопалов М.Ю.


ВЫПУСКНАЯ КВАЛИФИКАЦИОННАЯ РАБОТА
БАКАЛАВРА


Тема: ОПТИМИЗАЦИЯ ПРОСТРАНСТВА ПРОЕКТИРОВАНИЯ В HLS С ПОМОЩЬЮ МЕТОДОВ МАШИННОГО ОБУЧЕНИЯ


Студент				Темников И.О.
		подпись		
Руководитель	к.т.н., доцент каф. АПУ 			Каплун Д.И.
	(Уч. степень, уч. звание)	подпись		
Консультанты	д.э.н., профессор каф. ПЭ			Мосияш А.Э.
	(Уч. степень, уч. звание)	подпись		
	к.т.н., доцент каф. АПУ			Белаш О.Ю.
	(Уч. степень, уч. звание)	подпись		
	ассистент каф. АПУ			Ряскова Е.Б.
	(Уч. степень, уч. звание)	подпись		


Санкт-Петербург
2025
 
ЗАДАНИЕ
НА ВЫПУСКНУЮ КВАЛИФИКАЦИОННУЮ РАБОТУ

	Утверждаю
	Зав. кафедрой АПУ
	____________ Шестопалов М.Ю.
	«___»______________2025 г.
Студент	Темников И.О.		Группа	9371
Тема работы: Оптимизация пространства проектирования в HLS с помощью методов машинного обучения.
Место выполнения ВКР: каф. АПУ, СПбГЭТУ ЛЭТИ
Разработка и применение методов машинного обучения для оптимизации проектирования в высокоуровневом синтезе с акцентом на предсказание характеристик проектируемых систем с использованием графовых нейронных сетей и оптимизацию графов с помощью алгоритмов обучения с подкреплением
Содержание ВКР: 
Подготовка и обработка данных, разработка моделей машинного обучения экспериментальные исследования, составление бизнес-плана по коммерциализации результатов НИР.
Перечень отчетных материалов: пояснительная записка, иллюстративный материал.
Дополнительные разделы: Составление бизнес-плана по коммерциализации результатов НИР.

Дата выдачи задания	Дата представления ВКР к защите
«3» февраля 2025 г.	«29» мая 2025 г.
		
Студент		Темников И.О.
Руководитель    к.т.н., доцент каф. АПУ		Каплун Д.И.
 
КАЛЕНДАРНЫЙ ПЛАН ВЫПОЛНЕНИЯ 
ВЫПУСКНОЙ КВАЛИФИКАЦИОННОЙ РАБОТЫ

	Утверждаю
	Зав. кафедрой АПУ
	________ Шестопалов М.Ю.
	«___»______________2025 г.

Студент	Темников И.О.		Группа	9371
Тема работы: Оптимизация пространства проектирования в HLS с помощью методов машинного обучения.

Задача	Срок выполнения
Определение целей, задач и плана	3–7 февраля (Неделя 1)
Анализ методов оптимизации проектирования в HLS	8–14 февраля (Неделя 2)
Изучение возможностей применения машинного обучения	15–21 февраля (Неделя 3)
Синтез и обработка данных для обучения модели	22 февраля – 15 марта (Недели 4–8)
Формирование датасета и структурирование данных	16–28 марта (Недели 9–10)
Обучение графовых регрессионных моделей	29 марта – 5 апреля (Неделя 11)
Проектирование и обучение RL-модели	6–12 апреля (Неделя 12)
Тестирование и валидация результатов	12-20 апреля (Неделя 13)
Подготовка черновика ВКР	21 – 26 апреля (Неделя 14)
Анализ и интерпретация результатов эксперимента	27 апреля – 9 мая (Недели 15-16)
Подготовка финальной версии ВКР	10 -15 мая (Недели 16-17)

Студент		Темников И.О.
Руководитель    к.т.н., доцент каф. АПУ		Каплун Д.И.




РЕФЕРАТ
Работа направлена на разработку системы поддержки принятия проектных решений в области высокоуровневого синтеза. Система включает два взаимодополняющих компонента модуль предсказания характеристик проектируемых систем и модуль оптимизации проектных параметров. В качестве основы для предсказания используется архитектура графовых нейронных сетей, анализирующая структуру проекта, представленного в виде графа потока данных. Модуль оптимизации реализуется с применением алгоритмов обучения с подкреплением, обеспечивающих автоматизированный поиск проектных решений с учётом множества ограничений и критериев. Взаимодействие указанных компонентов позволяет формировать конфигурации проекта с заданными характеристиками в автоматическом режиме, что способствует сокращению времени разработки и снижению зависимости от ручного подбора параметров.
















ABSTRACT



























СОДЕРЖАНИЕ
ОПРЕДЕЛЕНИЯ, ОБОЗНАЧЕНИЯ И СОКРАЩЕНИЯ	7
ВВЕДЕНИЕ	9
1. ТЕОРЕТИЧЕСКИЕ ОСНОВЫ И ОБЗОР ЛИТЕРАТУРЫ	10
1.1. Актуальность исследования	10
1.2. Высокоуровневый синтез и его особенности	10
1.3. Графовые нейронные сети	12
1.4. Модуль распространения	13
1.5. Модуль выборки	15
1.6. Модуль обновления	16
1.7. Сравнение с классическими нейронными сетями	17
1.8. Обоснование эффективности для графовых структур в задачах HLS	17
1.9. Обучение с подкреплением в задачах оптимизации	19
1.10. Метод Actor-Critic	22
1.11. Обоснование эффективности применения RL в HLS	24
2. ПОДГОТОВКА И ОБРАБОТКА ДАННЫХ	26
2.1. Генерация тестовых случаев	26
2.2. Структура генерируемых данных	27
2.3. Организация процесса синтеза	30
2.2. Подготовка данных для машинного обучения в задачах HLS	32
2.3. Анализ и визуализация данных	36
3. РАЗРАБОТКА МОДЕЛЕЙ МАШИННОГО ОБУЧЕНИЯ	42
3.1. Архитектура графовых нейронных сетей	42
3.2. Модель предсказания LUT	42
Модель предсказания критического пути	49
Модель предсказания DSP	52
3.2. Система обучения с подкреплением	55
3.3.	Моделирование среды синтеза схемы	55
3.3.1.	Инициализация среды	55
3.3.2	Сброс среды	57
3.3.3.	Выполнение действий	58
3.3.4. Формирование состояния	58
3.3.5. Вычисление награды (_calculate_reward)	59
3.3.7. Архитектура модели ActorCritic	61
3.3.8. Система обучения модели Actor–Critic	63
4. ЭКСПЕРИМЕНТАЛЬНЫЕ ИССЛЕДОВАНИЯ	66
5. СОСТАВЛЕНИЕ БИЗНЕС-ПЛАНА ПО КОММЕРЦИАЛИЗАЦИИ РЕЗУЛЬТАТОВ НИР	69
5.1. Описание проекта (после названия параграфа, подпараграфа точки не ставятся. После всех параграов уберите точки)	69
5.1.1. Резюме	69
5.1.2. Описание продукции	70
5.1.3. Анализ рынка сбыта	71
5.1.4. Анализ конкурентов.	72
5.2. План маркетинга.	72
5.2.1. План продаж.	72
5.2.2. Товарная политика.	73
5.2.3. Ценовая политика.	74
5.2.4. Сбытовая политика и мероприятия.	75
5.3. План производства.	75
5.4. Финансовый план.	80

ОПРЕДЕЛЕНИЯ, ОБОЗНАЧЕНИЯ И СОКРАЩЕНИЯ
FPGA – Field-Programmable Gate Array (Программируемая логическая матрица) – интегральная схема, которую можно программировать для реализации различных логических функций.
HLS – High-Level Synthesis (Высокоуровневый синтез) – процесс преобразования алгоритмического описания системы на языках высокого уровня в аппаратное описание, которое можно использовать для синтеза цифровых схем.
LUT – Look-Up Table (Таблица поиска) – аппаратный блок, используемый для реализации логических функций путём хранения предварительно вычисленных значений.
DSP – Digital Signal Processor (Цифровой сигнальный процессор) – процессор, оптимизированный для обработки цифровых сигналов, используется в вычислительных задачах, требующих высокой скорости обработки данных.
CP – Critical Path (Критический путь) – последовательность операций в проектируемой системе, которая определяет минимальное время, необходимое для выполнения программы или системы.
RL – Reinforcement Learning (Обучение с подкреплением) – метод машинного обучения, в котором агент обучается путём взаимодействия с окружающей средой и получения награды за выполнение правильных действий.
GNN – Graph Neural Network (Графовая нейронная сеть) – тип нейронной сети, предназначенный для обработки графовых структур данных, где информация представлена в виде вершин и рёбер.
DFG – Data Flow Graph (Граф потока данных) – граф, представляющий зависимость данных между операциями в проектируемой системе. 
RTL – Register-Transfer Level (Уровень регистровых передач) – уровень проектирования цифровых схем, на котором работа системы описывается в терминах потоков данных между регистрами и операций, выполняемых над этими данными.
GPU – Graphics Processing Unit (Графический процессор) – специализированный процессор, предназначенный для высокопроизводительных параллельных вычислений, изначально разработанный для обработки графики, но теперь широко применяемый в машинном обучении, научных расчётах и других задачах, требующих массового параллелизма.
 
ВВЕДЕНИЕ
Современные системы проектирования аппаратного обеспечения сталкиваются с все более сложными решениями, связанными с улучшением производительности. Существующие инструменты для высокоуровневый синтеза позволяют преобразовывать алгоритмические описания в регистрово–транзисторные схемы, но они имеют ограничения в возможностях оптимизации.
Главной проблемой современных инструментов является отсутствие гибких механизмов многокритериальной оптимизации, а также значительные временные и вычислительные затраты на процесс синтеза. Данные ограничения существенно замедляют процесс разработки и повышают его стоимость.
В данной работе рассматривается решение указанных проблем методами внедрения в процесс синтеза алгоритмов машинного обучения для автоматизированной оптимизации распределения ресурсов. 
1. ТЕОРЕТИЧЕСКИЕ ОСНОВЫ И ОБЗОР ЛИТЕРАТУРЫ
1.1. Актуальность исследования
Актуальность исследования заключается в возрастающей сложности проектирования цифровых систем и отсутствия в существующих средствах высокоуровневого синтеза, алгоритмов оптимизации подбора параметров. При этом современные промышленные инструменты требуют значительных вычислительных ресурсов и времени на проведение итеративного синтеза, в задачах, предполагающих многокритериальную настройку. Для решения этих задач, в данной работе, были применены интеллектуальные методы анализа и оптимизации, способных учитывать структуру проекта.
В рамках данной работы реализуется методика генерации синтетических данных, разрабатывается архитектура моделей для анализа проектных графов и автоматизированного поиска параметров, осуществляется сбор и структурирование метрик, получаемых из инструментов синтеза, и проводится анализ эффективности подхода.

1.2. Высокоуровневый синтез и его особенности
Высокоуровневый синтез представляет собой технологию автоматического преобразования алгоритмического описания цифровой системы, на языках высокого уровня, например C, C++ или SystemC, в регистрово–транзисторное представление, необходимое для последующего логического синтеза и реализации на программируемых логических интегральных схемах или в виде интегральных микросхем. Основной задачей HLS является сокращение времени проектирования, повышение интерпретации проектного описания и облегчения повторного использования проектных решений.
При традиционном подходе к проектированию цифровых устройств много времени уходит на ручное написание и отладку RTL-кода. С использованием HLS можно меньше заботиться о реализации и сосредоточиться на правильной работе и структуре алгоритмов системы.
Высокоуровневый синтез разделен на четыре ключевых этапа, выполняемых последовательно [1]. На первом этапе осуществляется компиляция и анализ исходного кода, система HLS строит граф потока управления и данных (DFG), определяет зависимости между операциями и выявляет участки параллелизма. Второй этап – это планирование операций. На данном этапе система определяет порядок их исполнения. Он обеспечивает корректность между операциями и проводит подсчет затрачиваемого критического пути. Третий этап выполняет распределение ресурсов, где для логических операций сопоставляются конкретные аппаратные ресурсы, ALU, регистры, мультиплексоры и др. Завершающим этапом является генерация RTL–описания. Среда выполнения преобразует поведенческое описание в эквивалентный VHDL или Verilog–код, который может быть использован для реализации на аппаратной платформе.
Несмотря на данный функционал, систем высокоуровневого синтеза, существует ряд ограничений, препятствующих их повсеместному применению. Главной проблемой является длительность и ресурсоемкость выполнения синтеза. Из-за чего значительно увеличивается временные затраты на проектирование.  Другой немаловажной проблемой остаётся оптимизация аппаратных ресурсов программируемой логики. Одни оказываются перегруженными, другие используются не полностью, а также не учитывается стоимость вычислительные блоков, которые эксплуатируются нерационально. Существующие методы оперируют упрощёнными эвристиками и не учитывают взаимодействие между различными этапами синтеза. Данная проблема баланса особенно характерна для логических ячеек (LUT) и цифровых сигнальных процессоров (DSP). Также немаловажным является, что при использовании большого числа параметров и директив подбор оптимальной конфигурации может потребовать сотен запусков инструмента синтеза, каждый из которых занимает значительное количество времени.
С учетом этих ограничений, можно говорить о необходимости интеллектуальной автоматизации процессов HLS. Используя алгоритмы многокритериальной оптимизации, которые позволят находить баланс между требованиями к производительности и используемыми аппаратными ресурсами за короткое время.
Таким образом, высокоуровневый синтез представляет собой сложную технологию проектирования цифровых систем, которую можно улучшить, используя алгоритмы машинного обучения, что поможет сделать проектирование более эффективным, в частности для сложных современных цифровых устройств.
1.3. Графовые нейронные сети
Во многих областях данные эффективно представлять в виде графов. Это особенно актуально, для моделирования физических и математических процессов, например, в анализе поиска оптимального маршрута, цепи производства, а также анализ и обработка изображений. Простейшие структуры графов состоят из отдельных узлов и последовательностей, выстроенных в линию. Однако, для решения масштабных задач требуются более комплексные графовые организации, такие как древовидные структуры, а также ациклические и циклические графы. Решением для анализа таких графов являются графовые нейронные сети (Graph Neural Networks, GNN). Позволяющие, в отличие от классических нейронных сетей, где входные данные имеют фиксированную структуру, эффективно обрабатывать неэвклидовы структуры. А именно графы, не имеющие фиксированной топологии, где расстояние между узлами задается не метрикой R^n, а конкретными путями в графе.
Основной принцип GNN заключается в агрегации информации от соседних вершин в графе, мы можем сохранять сведения о вершинах, рёбрах и всём графе в целом, сохраняя эмбеддинги для соответствующих элементов. Это осуществляется итеративно в процессе распространения сообщений, в ходе которого каждая вершина получает информацию от своей локальной окрестности (1), 
h_ʋ^k=〖UPDATE〗^k (h_ʋ^(k-1)  ,〖AGGREGATE〗^k ({h_u^(k-1):u∈N(ʋ)})) ,		(1)
где h_ʋ^k— представление ʋ вершины на k–ом слое, N(ʋ)— множество соседей вершины, AGGREGATE и UPDATE — параметризованные функции агрегации и обновления соответственно.
В графовых сетях есть 3 основных модуля [2], с помощью которых строится архитектура, представленных на рисунке 1 
	Модуль распределения (propagation)
	Модуль выборки (sampling)
	Модуль обновления (pooling)
 
Рисунок 1 – Структурная схема архитектуры графовой нейронной сети (GNN)
1.4. Модуль распространения
Для модуля распространения наиболее часто используются сверточные операторы. Их задача — обобщить классическую операцию свёртки, на структуры графов. Существуют два основных подхода к построению таких операторов: спектральный и пространственный.
Спектральные подходы основываются на спектральном представлении графа, а именно — на спектре нормализованного лапласиана графа
 L=I-D^(-1/2) 〖AD〗^(-1/2),				(2)
где A — матрица смежности, а D — диагональная матрица степеней. Используя графовое преобразование Фурье (3, 4), сигнал x, заданный на узлах, переводится в спектральную область с помощью матрицы собственных векторов U, после чего к нему применяется фильтр g, и сигнал возвращается обратно (5)
F(x) = U^T x,				(3)
F^{-1}(x) = Ux,				(4)
g ⋆x = U ⋅g_w⋅U^T x,			(5)
где g_w — диагональная матрица параметров фильтра, определяющая поведение свёртки. Такая операция обладает высокой теоретической обоснованностью, однако в классическом виде она неэффективна для масштабных графов.
Наиболее широко используемым пространственным оператором является GCN, предложенный Kipf и Welling (2017). Они упростили ранее используемое разложение ChebNet (6), использующий полиномы Чебышева для аппроксимации фильтра, ограничив порядок полинома K=1, и предложили использовать симметрично нормализованную матрицу смежности с добавлением связей узлов с самими собой.
g ⋆x ≈∑_(k=0)^K▒〖w_k T_k (L ̃ )x〗	(6)
где T_k полиномы Чебышёва, а L ̃— масштабированный лапласиан. 
A ̂= A + I		(7)
(D_ii ) ̂= ∑_j▒(A_ij ) ̂ 	(8)
H = D ̂^(-1/2) A ̂D ̂^(-1/2) XW	(9)
где X — входные признаки узлов, W — обучаемые параметры.

1.5. Модуль выборки
С увеличением глубины графовой нейросети возрастает количество соседей, от которых требуется агрегировать информацию. Данное явление называется взрыв соседей (neighborhood explosion) []. При работе с крупными графами становится невозможным хранить и обрабатывать всю информацию о локальной окрестности каждого узла. Чтобы избежать данной проблемы применяется модуль выборки, позволяющий масштабировать GNN-модели. Существуют три основных подхода к выборке: выборка узлов, выборка слоёв и выборка подграфов.
Для этого рассмотрим модель GraphSAGE, использующую агрегацию, опирающуюся на случайное подмножество соседних узлов. Этот подход позволяет регулировать максимальное количество соседей, которое будет учтено. Общая формула, используемая в GraphSAGE для агрегации, представлена ниже
h_(N(ʋ))^(t+1)=〖AGGREGATE〗^k ({h_u^t:u∈N(ʋ)})),		(9)
h_ʋ^(t+1)=σ(W^(t+1)*[h_u^t  ||  h_(N(ʋ))^(t+1)]),		(10)
где AGG – агрегирующая функция (например, среднее или LSTM), ∥ –конкатенация, σ – функция активации (например, ReLU), а W^(t+1) — обучаемая матрица весов.
Другой же подход – это выборка по слоям, алгоритм обучения которого представлен на рисунке 2. Он позволяет сохранять небольшой набор узлов для агрегирования в каждом слое, позволяя контролировать коэффициент расширения. FastGCN [] Напрямую определяет рецептивное поле для каждого слоя. Он использует выборку по важности, при которой важные узлы с большей вероятностью попадают в выборку ().

 
Рисунок 2 – Алгоритм обучения GNN с выборкой по слоям
	Альтернативный способ вместо выборки узлов и ребер, является выборка подграфов (subgraph sampling). В этом случае модель обучается не на всей структуре графа, а на отдельных его фрагментах. Так, в Cluster-GCN [] сначала производится кластеризация графа, а затем обучение проводится на уровнях отдельных кластеров.
1.6. Модуль обновления
В традиционных сверточных нейронных сетях (CNN) операции свёртки обычно сопровождаются понижением размерности с помощью pooling-слоёв, что позволяет выделять более общие и устойчивые признаки. Аналогичная задача стоит и в графовых нейросетях: при работе с крупными и структурно насыщенными графами требуется агрегировать локальные представления узлов в глобальные признаки подграфов или всего графа.
Pooling-модули в графовых нейронных сетях подразделяются на прямые (direct pooling) и иерархические (hierarchical pooling). Первый тип предполагает агрегирование признаков узлов напрямую, без изменения структуры графа. Второй тип формирует иерархические представления графа путём его поэтапного укрупнения и сведения.
Прямой pooling (Direct pooling) агрегируют признаки узлов в один вектор с использованием операций, инвариантных к порядку mean, max, sum а также механизмов внимания
Иерархический pooling формируют представление графа в несколько этапов, постепенно понижая его размерность и изменяя структуру. Одним из наиболее часто используемых подходов является DiffPool [], где в каждом слое обучается матрица назначений S^t, определяющая принадлежность узлов к кластерам. Эмбеддинги и структура графа обновляются следующим образом:
S^t=softmax (〖GNN〗_pool (A^t,H^t )),		(11)
A^(t+1)=〖(S^t)〗^T A^t S^t,				(12)
H^(t+1)=〖(S^t)〗^T H^t					(13)
где A^t и H^t – соответственно матрица смежности и матрица признаков на t-ом уровне.
1.7. Сравнение с классическими нейронными сетями
Классические нейросети, такие как полносвязные (MLP) и CNN, предполагают регулярную структуру входных данных. GNN, напротив, допускает произвольную структуру связей, что критически важно для представления сложных зависимостей в графоподобных данных.
Характеристика	Классические НС	GNN
Структура входных данных	Регулярная	Произвольная (граф)
Пространственные зависимости	Локальные (CNN)	Локальные и глобальные
Инвариантность к перестановке	Нет	Да (при симметричной агрегации)

1.8. Обоснование эффективности для графовых структур в задачах HLS
В задачах HLS, для оптимизации аппаратной реализации на уровне поведенческого описания, данные естественным образом выражаются в виде графов потока данных. GNN позволяет моделировать сложные зависимости между операциями, параллелизм, зависимости и ресурсоемкость операций.
Для предсказания характеристик схем и, в частности, времени выполнения, GNN уже показало высокую точность благодаря способности обобщать локальные шаблоны в более глобальные зависимости. Модели типа GCN позволяют извлекать высокоуровневые представления графов схем, что позволяет их использовать для анализ узких мест (bottleneck identification).
Графовые нейронные сети — это класс архитектур, специально разработанные для обработки структурированных данных, организованных в форме графов. GNN учитывает произвольную топологию связей между элементами. Что делает их особенно эффективными в задачах, где пространственная или логическая структура данных имеет решающее значение. Высокоуровневый синтез цифровых схем является именно такой задачей, где топология взаимодействия между компонентами определяет функциональность и характеристики результирующей аппаратной реализации. 
Применение GNN в высокоуровневом синтезе предполагает представление цифровой схемы в виде графа потока данных. Для этой модели вершины графа представляют собой отдельные операции, логические или арифметические вычисления, а рёбра — потоки данных между ними. Такое представление возможно получить из промежуточного представления кода, генерируемого компилятором HLS. Описание узлов должно включает в себя следующие принципы: тип операции, её вычислительную стоимость, ширину битности и другие характеристики, влияющие на синтез. Связи между узлами могут быть дополнены признаками, отражающими направление потока управления.
Предсказание характеристик схем на основе GNN позволяет оценивать метрики, такие как количество используемых LUT, DSP–блоков и длину критического пути. Модель обучаются на выборках, полученных из синтезированных схем, для которых известны значения целевых метрик. Векторные представления графов используются в качестве входа модели, а на выходе формируются числовые оценки. Благодаря чему можно производить оценку качества решений без выполнения полного синтеза, что существенно ускоряет цикл проектирования. Чтобы улучшить предсказательную способность таких моделей их архитектура может включать определенные эвристики. Для LUT можно реализовать несколько слоёв графовой обработки с последующим глобальным пулингом и регрессионным выходом. Для предсказания количества DSP особое внимание уделить специфике узлов, на которых, выполняется умножение, через добавление специальных признаков и модулей внимания. Для предсказания CP потребуется внимание к глубине и каскадности операций, что возможно сделать через использование расширенных признаков рёбер и увеличения глубины архитектуры сети. В качестве функций потерь можно применять среднеквадратичную ошибку или абсолютное отклонение. А также, для повышения обобщающей способности применить методы регуляризации, dropout и нормализацию по батчам.
Также важно помнить, что проблемой нейронных сетей остается трудность понимания принимаемых ей решений, так как сложность модели не дает обоснованного понимания какие именно элементы схемы повлияли на тот или иной результат. Помимо этого, обучение CNN-моделей требует значительных вычислительных ресурсов, что ограничивает возможность их обучения для слабой инфраструктуры. 
1.9. Обучение с подкреплением в задачах оптимизации
Обучение с подкреплением (Reinforcement Learning, RL) является парадигмой машинного обучения, где агент обучается принимать решения, взаимодействуя с окружающей средой с целью получения максимальной суммарной награды. В отличие от обучения с учителем, где модель обучается на размеченных данных, RL не указывают, какие именно конкретные действия необходимо выполнять для получения желаемого результата, а вместо этого он сам должен определить, что именно приносит наибольшее вознаграждение, пробуя их. Агент выбирает действия, после одного варианта, получает награду от среды и пробует следующий вариант. Именно поиск методом проб и ошибок и отсроченное вознаграждение – являются двумя наиболее важными отличительными особенностями обучения с подкреплением.
Основные компоненты отраженные на рисунке 3, характеризующие RL, содержат следующие элементы:
	Агент – автономный агент, который принимает решения и обучается.
	Среда – система или контекст, в котором агент действует и обучается. Она определяет правила, динамику и механизмы обратной связи.
	Состояние – текущее состояние системы, в которой находится агент.
	Действие – выбор, который делает агент для текущего состояния.
	Награда – скалярный сигнал, получаемый агентом после выполнения действия.
 
Рисунок 1 – Схема взаимодействия агента и среды в обучении с подкреплением
Процесс принятия решений в RL можно сформулировать как задачу управления в марковском процессе принятия решений (Markov Decision Process, MDP), где следующее состояние зависит только от текущего, а не от всей истории. Формально MDP определяется пятеркой (S,A.P,R γ), где S множество состояний, A множество действий, P(s`|s,a) вероятность перехода от состояния s в состояние s` при выполнении действия a, R(s,a) ожидаемая награда за данное действие. 
 Определение политики π работы модели заключается в том, что это некоторый набор правил, который управляет поведением агента. Формально определяется как распределение вероятностей по действиям для каждого возможного состояния:
π(a│s)=P[A_t=a|S_t=s],		(14)
Где π(a│s) вероятность выбора между действиями a и s, A_t действие, выбранное агентом в момент времени t, S_t состояние среды в момент времени t.
И цель агента максимизировать целеву функцию, представленное в виде выражения качества, через ожидаемую награду
J(π)= E_π [∑_(t=0)^inf▒〖γ^t r_t]〗,		(15)
Где r_t награда в момент времени t, γ ∈ [0,1) – коэффициент дисконтирования, отражающий предпочтение краткосрочной выгоды по сравнению с долгосрочным, E_π – ожидание.
Для оценки качества различных политик применяется функция ценности, которая определяет ожидаемую суммарную награду из состояния s и следовании политике π
V^π (s)= E_π [∑_(t=0)^inf▒〖γ^t R_t | S_(0 )=s]〗		(16)
Для задач, где агенту необходимо знать, насколько выгодно ему находиться в том или ином состоянии применяется более обобщенная функция ценности действия
Q^π (s,a)= E_π [∑_(t=0)^inf▒〖γ^t R_t | S_(0 )=s,A_(0 )=a]〗	(17)
В отличие от предыдущей формулы, данная функция позволяет оценить каждое действие отдельно. И выбрать наиболее значимое действие.
Прямой расчет ценности не является эффективным, особенно в задачах с большим пространством состояний. Для упрощения этой задачи используется рекурсивная формализация, именуемая как уравнение Беллмана. Данное уравнение позволяет выразить ценность состояния не через расчёт сумм последующих наград, а через ценность ближайших шагов.
V^π (s)= ∑_(a∈A )▒〖π(a│s) ∑_(s`∈S)▒〖P(s`┤|  s,a)[R(s,a)+〗 γV^π (s`)]〗	,	(18)
Для функции ценности действий уравнение Беллмана формулируется аналогично, но включает непосредственно выбранное действие
Q^π (s,a)= R(s,a)+γ∑_(s`∈S )▒〖P(s`┤|  s,a)∑_(a`∈S)▒〖π(a`┤|  s`)Q^π (s`,a`)〗〗,	(19)
В этом случае ценность пары (s,a)(s, a)(s,a) определяется как немедленная награда и ожидаемая ценность всех возможных последующих действий в следующем состоянии, выбранных в соответствии с политикой.
Отличительной особенностью RL является способность обучаться в среде с частичной информацией и отложенным вознаграждением, что делает его особенно подходящим для задач, связанных с оптимизацией процессов и ресурсов.
1.10. Метод Actor-Critic
Одним из наиболее эффективных алгоритмов RL является метод Actor–Critic, который сочетает в себе преимущества политико–ориентированных и ценностно–ориентированных подходов. Его структура состоит из двух моделей, актера, который реализует стратегию выбора действий, и критика, который оценивает ценность состояний. А именно, в более формальнов виде, актор реализует политику π(a ┤|s;θ)  параметризованную вектором θ, в то время как критик аппроксимирует функцию ценности V^π (s; w), либо действия Q^π (s,a;w)  c помощью параметров w. Где θ и w веса сети, обновляющиеся в процессе обучения.
Основная идея алгоритма заключается в использовании градиента политики для обновления параметров актора, где направление улучшения определяется оценкой критика. Таким образом, актор отвечает за обучение стратегии, а критик за оценку последствий этой стратегии. На рисунке 3 иллюстрируется взаимодействие между актором и критиком.
 
Рисунок 3 – Схема взаимодействия компонентов в алгоритме Actor–Critic
 Обновление параметров актора происходит в направлении градиента усиленной награды, взвешенной ошибкой критика.
∇_θ J(π_θ) = E_πθ  [ ∇_θ  log⁡π_θ (a | s) ⋅ A^π (s,a) ]	(20)
где A^π – функция преимущества (advantage function), определяющая, насколько действие a в состоянии s лучше среднего действия в этом состоянии. В качестве приближённой оценки A^π (s,a) применяется выражение:
A^π (s,a)= Q^π (s,a)-V^π (s) ,			(21)
Критик в этой модели учится с использованием градиентного спуска, что, в свою очередь, помогает модели уменьшить ошибки в предсказании ценности. Такая гибридная структура модели позволяет сделать обучение более стабильным. Благодаря этому данная модель обладает свойствами быстро меняться и подстраиваться под изменения в окружении. Так как критик, помогает справляться с шумом во время обучения. Эти качества делают модель полезной в ситуациях с непрерывными пространством и огромной размерностью возможных состояний.
1.11. Обоснование эффективности применения RL в HLS
В контексте HLS задача оптимизации может быть естественным образом сформулирована как задача RL. Так, например, агент может учиться выбирать последовательность ресурсных назначений (в данной работе это LUT, DSP, CP). Пространство состояний описывает текущую структуру схемы, а действия соответствуют операциям, таким как выбор ресурса для реализации операции для данного конкретного узла.
Пространство состояний кодируется вектором признаков, топологическую информацию, типы операций, ресурсоемкость и текущие назначения. Функция награды формируется как взвешенная комбинация нескольких метрик количества LUT, числа DSP–блоков, длины критического пути, что позволяет реализовать многокритериальную оптимизацию. 
r= α⋅  LUT/〖LUT〗_ref + β⋅  DSP/〖DSP〗_ref + γ⋅  CP/〖CP〗_ref ,		(22)
где  α, β, γ коэффициенты, отражающие приоритеты конкретной оптимизации, а x/x_ref  нормализация значений.
В реализации модели агент взаимодействует с симулированной средой RLEnv, таким образом мы эмулируем процесс распределения ресурсов в HLS. Среда должна иметь функциональность для представления текущего состояния схемы, применения действий и оценки последствий. 
Процесс выбора действия должен основываться на стохастической политике, через сэмплирование из распределения π(a ┤|s;θ)  для балансировки исследования и эксплуатации. В процессе обучения параметры актора и критика обновляются синхронно на основе накопленных эпизодов.
Для оценки эффективности подхода используются метрики, получаемые на ранее переобученных моделях GNN, они определяют какими значениями DSP, LUT и CP обладает схема. Такой подход позволит искать оптимальные политики без необходимости проведения каждый раз нового синтеза HLS, для оценки результата, а также находить сбалансированные решения и адаптироваться к новым архитектурам схем без ручной настройки.
Но важно помнить, что для практического применения Actor–Critic необходимо учесть высокую чувствительность модели к параметрам награды коэффициента α, β, γ необходимо настраивать вручную, чтобы достичь необходимых результатов. Для обучения необходимо множество эпизодов до стабилизации награды, а также модель тяжело обобщается на ранее невиданных структурах. 
 
2. ПОДГОТОВКА И ОБРАБОТКА ДАННЫХ
2.1. Генерация тестовых случаев
Цель данной работы является реализация алгоритма оптимизации подбора параметров синтеза, при которой модель будет оптимизировать вычисления применяя LUT или DSP для операций умножения, при этом стараясь сокращать время критического пути, без прямого использования инструментов HLS. В данной работы был выбран метод сбора данных на случайных графах. Для этого был разработан алгоритм, позволяющий автоматизировать генерацию схем и сбор метрик, используя Vitis HLS для синтеза. 
На первом этапе определяется разрядность данных, для данной работы были выбраны от 2 до 32 бит. Количество входов для каждого тестового случая выбирается случайно, для входов от 5 до максимального указанного пользователем (30 в данной работе). 
Важным этапом является формирование связей, ребер между узлами графа. Здесь каждая операция принимает строго два входа, что соответствует семантике бинарных арифметических операций. Связи между узлами формируются с учётом направления вычислений, то есть при построении становится невозможным появление циклов. Дополнительно контролируется достижимость выходных узлов, так как выходами схемы становятся те вершины графа, которые не используются в качестве операндов другими операциями.
Таким образом производится инициализация всего графа и его структуры в виде списка all_ops. В нем, каждая операция представлена объектом, содержащим информацию о типе, входах, выходах, разрядности и идентификаторе. Рассмотрим подробнее, all_ops = [i, type, prec, out_degree], где i номер узла, type его тип вход/выход/сложение/умножение, prec разрядность в битах, out_degree количество исходящих ребер.
Данный список в дальнейшем будет использован для написания и генерации тестовых случаев DFG, а также .cc для Vitis HLS соответственно.
2.2. Структура генерируемых данных
В разработанной системе каждый тестовый случай представляет собой синтетически сгенерированный набор файлов, содержащий полное описание DFG, соответствующее ему C++ представление для HLS, а также необходимые директивы и сценарии автоматической сборки. Структура данных каждого кейса организована в отдельной директории case_X, где X — уникальный идентификатор теста.
Основным элементом является файл DFG_case_X.txt, содержащий описание графа. Он включает четыре ключевые секции 
	Primary Inputs, 
	Intermediate Operations, 
	Edges 
	Primary Outputs. 
Входные узлы в Primary Inputs описываются в формате inX INTY, где X номер входа, а Y разрядность сигнала (в диапазоне от 2 до 16 бит). Операции Intermediate Operations, внутри графа, представлены в виде mX op INTY, где op — операция сложения (+) или умножения (*). Связи между узлами Edges фиксируются в виде пар идентификаторов, определяющих направление передачи данных in10 m25, m25 m29 и т.д. А выходы указываются в финальной секции списком узлов o214.
Пример сгенерированного графа (фрагмент DFG_case_1.txt)
# Primary Inputs:
in1 INT8
in2 INT12
...
# Intermediate Operations:
m15 + INT10
m16 * INT14
...
# Edges:
in1 m15
in2 m15
m15 m16
...
# Primary Outputs:
o45
o46
Файл case_X.cc необходим для синтеза в Vitis HLS, он содержит реализацию вычислений на C++. Структура данного файла включает объявление функций верхнего уровня, массивов входных и выходных данных, а также директивы array_partition для параллелизма, они указывают, что входные и выходные массивы должны быть разделены. После этого описывается пошаговое определение всех промежуточных операций аналогично графовой структуре. Типы данных узлов формируются на основе разрядности, созданных во время генерации, и задаются с помощью ap_int<Y>.
#include <stdio.h>
#include "ap_fixed.h"

void case_1(
    ap_int<16> in_data[24],
    ap_int<16> out_data[29]
)
{
#pragma HLS array_partition variable=in_data complete
#pragma HLS array_partition variable=out_data complete

ap_int<15> in1;
in1.range(14, 0) = in_data[0].range(14, 0);
…
ap_int<10> in3;
in3.range(9, 0) = in_data[2].range(9, 0);
…
}

Файл directive.tcl используется для задания директив компилятору HLS. Он позволяет задавать на каких именно ресурсах (LUT или DSP) выполняется операций умножения.
set_directive_resource –core Mul_LUT "case_1" m25
set_directive_resource –core Mul_LUT "case_1" m26
set_directive_resource –core Mul_LUT "case_1" m27
set_directive_resource –core Mul_LUT "case_1" m28
set_directive_resource –core Mul_LUT "case_1" m29
…
В данном случае для узлов m25 – m29, которые содержат операцию умножения. И с помощью set_directive_resource умножение будет назначено на LUT.
Последний файл, script.tcl позволяет автоматизировать создание проектов для синтеза в Vitis HLS. Скрипт описывает команды для создания проекта, создание необходимых для синтеза исходных файлов, выбора целевой платформы (для исследования применялась виртуальная плата xc7z020clg484–1), установка частоты тактового сигнала и запуск процедуры синтеза с последующим экспортом результатов.
open_project –reset project_tmp
set_top case_1
add_files case_1.cc
open_solution –reset "solution_tmp"
set_part {xc7z020clg484–1}
create_clock –period 10 –name default
source "./directive_tmp.tcl"
csynth_design
export_design –evaluate verilog –format ip_catalog
exit
Таким образом обеспечивается полный пакет компонент, от текстового описания DFG до управляющих скриптов, для обеспечения корректности и воспроизводимости процесса синтеза и анализа рассматриваемых далее.
2.3. Организация процесса синтеза
В данной работе были созданы 40 графов, представляющих различные варианты вычислительных схем. Эти графы являются основой для сбора обучающих данных для построения и обучения модели. Для того, чтобы получить метрики, на которых модель может обучаться, необходимо для каждого графа провести серию запусков синтеза с различными конфигурациями директив directive.tcl, которые задают умножений либо на LUT, либо на DSP-блоки.
Для этого был реализован модуль run_one_case.py, который обрабатывает один кейс, начиная от генерации комбинаций директив и до извлечения и сохранения результатов синтеза. Скрипт запускает Vitis HLS на сформированных файлах, и после каждого синтеза изменяет директивы назначения ресурсов. Это реализовано с помощью случайного комментирования строк директив в файле directive.tcl. Это позволяет разнообразить обучающие примеры для дальнейшего обучения моделей.
Таблица 1 – «Анализ конкурентов»
i=1	random.uniform(0, 5) 	вероятность комментирования 60%
i=2	random.uniform(0, 6)	 вероятность комментирования 50%
i=16	random.uniform(0, 20)	вероятность комментирования 15%
i=17	random.uniform(0, 4)	вероятность комментирования 75%

Каждый запуск синтеза осуществляется в изолированной среде, формируются новые файлы скриптов и директив для каждого нового синтеза, создаются временные проекты и решения, необходимые Vitis HLS для компиляции и оценки схемы. После завершения синтеза из отчётов (_export.rpt) извлекаются ключевые аппаратные метрики, количество использованных логических ячеек (SLICE), число LUT и FF, количество DSP-блоков, а также значения CP — как после синтеза, так и после реализации.

Для каждого тестового случая сгенерированной выборки (case_1, case_2, и т.д.) выполняется серия запусков синтеза с различными конфигурациями директив. Входными параметрами системы управления являются
	num_cases количество кейсов для обработки в рамках одного запуска
	samples число уникальных комбинаций директив, синтезируемых для каждого кейса
	start_solution порядковый номер решения, с которого начинается синтез
	case_start идентификатор первого обрабатываемого кейса
Из-за больших вычислительных и временных затрат в данной работе были синтезированы решения для 40 уникальных графов с различной комбинацией директив от 50 до 100 вариантов.
Все результаты структурируются и сохраняются в формате JSON-файла case_X_all_data.json, где X — номер кейса. Файл обновляется после каждого успешного синтеза и содержит используемые директивы получаемые из directive.tcl, назначенные операции на LUT, а также соответствующие метрики из отчета синтеза. Рассмотрим структуру решения подробнее:
{
  …
  "solution_2": { 
    "directives": [
	"# set_directive_resource –core Mul_LUT \"case_1\" m33",
      "set_directive_resource –core Mul_LUT \"case_1\" m34",
      "set_directive_resource –core Mul_LUT \"case_1\" m36",
…
],
"LUT_op": [
    	  34,
        35,
        36,
        37,
  …
],
      "SLICE": 1045,
      "LUT": 3125,
      "FF": 910,
      "DSP": 38,
      "CP_post_synthesis": 7.832,
      "CP_post_implementation": 8.68,
      "CP_hls_metadata": 0
  },
…
}
Такой подход обеспечивает разнообразие конфигураций, от почти полностью закомментированных до минимально измененных вариантов, что важно для исследования пространства возможных решений в RL.
В данной работе также был сгенерирован .bat–файл для корректной настройки окружения Vitis HLS. Это связанно с особенностью выполнения среды в операционной системе Windows. Для этого из корня Vitis HLS определяется и копируется исполняемый файл tee.exe, необходимый для перенаправления и анализа вывода процесса компиляции. Также синтеза осуществляется в виде отдельного подпроцесса, для которого реализован модуль контроля активности вывода. В случае отсутствия вывода в течение заданного времени (по умолчанию 5 минут), процесс принудительно завершается, предотвращая зависания.
При наличии ошибок, связанных с отсутствием отчетов или сбоем синтеза, система автоматически переходит к следующей конфигурации, сохраняя успешные результаты.
2.2. Подготовка данных для машинного обучения в задачах HLS
Для применения методов графового машинного обучения требуется специальная подготовка данных, которая будет отражать вычислительную структуру алгоритма и характеристики аппаратной реализации. Для этого данные обрабатываются в два этапа, data_preprocess.py и generate_graph_datasets.py. Такое разделение обусловлено необходимостью сначала привести исходные данные (DFG–файлы и json–метрики) к унифицированному, а именно побитовому, промежуточному представлению, а затем сформировать на их основе графовые структуры, которые будут пригодны для обучения нейросетевых моделей.
Так, модуль data_preprocess.py начинает работу сканируя директории с кейсами и для каждого доступного кейса, а именно которые содержат не пустые DFG и json файлы. Он производит парсинг DFG–графов, извлекает информацию о типах операций, формирует список рёбер, а в конце переводит структуру данных в битовую последовательность, сохраняя ее в формате pickle. Эти действия дают возможность гарантировать соответствие данных и стандартизацию в вычислительных графах.
Рассмотрим подробнее преобразование в битовую последовательность. Для каждого узла графа применяется кодирование, представляющая собой вектор из 11 бинарных признаков. Первые четыре бита вектора указывают на принадлежность узла к определенной операции вход, выход, операция сложения или операция умножения. Следующие пять бит соответствуют разрядности данных. Биты кодируются не линейно, а с учётом распределённого веса и значимости — например, INT2 представляется как 00001, INT4 как 00011, INT8 как 00111, INT16 как 01111, и INT32 как 11111. Последние два признака отражают аппаратные свойства для операций умножения и дополнительная логическая метка, зарезервированная для возможного улучшения моделей. Более подробная структуру этого вектора:
Первые 4 бита [0,0,0,1] – тип узла,
	[1,0,0,0] – входной узел
	[0,1,0,0] – операция сложения
	[0,0,1,0] – операция умножения
	[0,0,0,1] – выходной узел
Следующие 5 бит [0,0,0,0,0] – бинарное представление точности (для выходов всегда 0), [0,0,1,1,1] – бинарное представление разрядности 8
Последние 2 бита [0,0] – дополнительные признаки, отображающие аппаратную реализацию, таким образом, например, последний бит используется для идентификации LUT или DSP для умножения, но назначается на следующем этапе.
После завершения кодирования узлов следующим этапом является формирование рёбер графа. Здесь каждое ребро представляется как направленная связь от источника к приёмнику, что соответствует потоку данных. Полученная информация сохраняется в виде таблицы, где каждая строка содержит идентификаторы начального и конечного узлов. 
Далее производится извлечение и структурирование метаинформации о графе, которая содержит общее количество входов, выходов, промежуточных операций, количество операций умножения и общее количество рёбер. Эта информация необходима в дальнейшем, для дальнейшего анализа и визуализации графов.
В итоге, в результате работы модуля получаем набор промежуточных данных, сохранённых в побитовом виде pickle и фактические данные в виде DataFrame-ов. Эти наборы содержат признаки узлов, структуру рёбер, метаданные о графе, а также аппаратные метрики, полученные из результатов синтеза. 
Второй модуль generate_graph_datasets.py использует полученные структуры для построения обучающего графового набора, а именно структурирует их для подачи на вход нейросетевым моделям GNN. Для каждого доступного кейса выполняется загрузка всех необходимых компонентов таблицы узлов с бинарными признаками, списка рёбер, отображения номеров узлов, индексов операций умножения, а также сопутствующих аппаратных метрик, полученных ранее. Для каждого кейса создается несколько вариантов синтеза, полученных при использовании различных комбинаций директив. Поэтому для одного и того же графа создаются несколько его версий, отличающихся только одним дополнительным признаком (f10), отражающим назначение ресурса для каждой операции умножения, то есть f10 = 0 для DSP, иначе LUT. Это позволяет представить одну и ту же структурную схему в различных конфигурациях реализации, расширяя обучающую выборку и позволяя модели учиться на множественных вариантах аппаратной реализации.
Для каждой конфигурации формируется объект графа CustomGraph. Основной задачей данного класса является преобразование табличных представлений узлов и рёбер в числовые массивы фиксированного формата, пригодные для подачи в GNN. Его структура состоит из трех атрибутов 
	Матрица признаков узлов (node_features). 
Она строится на основе таблицы nodes (DataFrame), где каждый узел представлен строкой, содержащей бинарный вектор длины 11. 
	Массив индексов рёбер (edge_index) 
Он отражает направленные связи между узлами. Для этого используется таблица edges, в которой каждая строка указывает на пару узлов источник – приёмник. Здесь строится отображение всех идентификаторов узлов в их порядковые индексы в массиве признаков. Далее, с помощью этого отображения каждая строка edges преобразуется в пару чисел индексов соответствующих узлов. Все такие пары объединяются в двумерный массив размера 2 × E, где E — общее количество рёбер.
	Словарь сопоставления идентификаторов узлов и их индексов (node_mapping).
Это обеспечивает однозначную навигацию по графу и позволяет при необходимости интерпретировать выходы модели или восстанавливать из индексов в исходные обозначения узлов (in1, m25 или o46).
Также класс предоставляет методы для извлечения всех ключевых элементов графа (инкапсуляция). И в завершение все графы и их метки сохраняются в сериализованном виде в graph_dataset.pkl.
Таким образом, два модуля дополняют друг друга, реализуя сквозной процесс подготовки данных, от необработанных входных файлов до обучающей выборки, пригодной для GNN–моделей. 
На рисунках 4 и 5 представлены примеры одной графовой структур, полученных в результате этапа подготовки данных. Цветовое кодирование узлов визуализирует различные типы операций, входы и выходы. Первый вариант (круговая раскладка) наглядно демонстрирует плотность связей и сложность структуры, а второй (раскладка по уровням) — поток данных от входов к выходам.
 
Рисунок 4 – Пример вычислительного графа после преобразования (круговая раскладка)
 
Рисунок 5 – Пример вычислительного графа в раскладке по уровням

2.3. Анализ и визуализация данных
Для данной работы, подготовленный набор данных содержит 2125 уникальных реализаций вычислительных графов, каждая из которых соответствует конкретной конфигурации директив синтеза и отличается значениями аппаратных метрик. Объем выборки обусловлен как сложностью процесса генерации, так и практическими ограничениями использования инструментов синтеза. Далее приводится статистика по самим графам и их синтезированным метрикам.
Таблица 1 – «Анализ конкурентов»
Метрика	Минимум	Максимум	Среднее	Ст. отклонение
Количество узлов	58,0	210,0	155,7	58,3
Входы	5,0	30,0	17,0	7,3
Выходы	8,0	32,0	19,0	8,6
Количество операций	35,0	189,0	118,5	48,1
Узлы сложения	7,0	48,0	23,2	9,9
Узлы умножения	25,0	153,0	95,3	39,2

Статистика по метрикам
Таблица 1 – «Анализ конкурентов»
Метрика	Среднее значение	Медиана	Диапазон
LUT (Look–Up Table)	1801	1547	33 – 5324
DSP (Digital Signal Processing)	10.5	7	0 – 62
CP_synthesis (ns)	6.98	7.10	3.03 – 12.30
CP_implementation (ns)	7.66	7.87	2.67 – 12.89

Как видно из таблицы, показатели ресурсов LUT и DSP варьируются в широком диапазоне, что указывает на значительное разнообразие реализованных графов. Примеры с низким количеством ресурсов свидетельствуют о компактных реализациях, тогда как верхние границы диапазонов соответствуют ресурсоемким проектам. Это важно для обучения моделей, способных охватывать как оптимизированные, так и экстремальные случаи. Значения CP также охватывают характерный диапазон для задач HLS, с медианой около 7 нс. Более подробное распределение характеристик представлено на следующих представленных диаграммах на рисунках 6-9.
 
Рисунок 6 – Распределение значений LUT
Распределение количества используемых LUT по всем графам. Видно, что большинство реализаций используют от 0 до 2000 LUT, однако встречаются и более ресурсоёмкие варианты. Среднее значение превышает медиану, что указывает на наличие правосторонних выбросов.
Распределение количества используемых DSP представлена на рисунке 7. Основная масса графов использует менее 15 DSP, но встречаются отдельные случаи с существенно большим потреблением. 
 
Рисунок 7 – Распределение значений DSP
 
Рисунок 8 – Распределение критического пути (CP_implementation)
Большинство графов имеют критический путь в диапазоне 6–9 нс, что соответствует типичным значениям для задач HLS. Распределение близко к нормальному.
 
Рисунок 9 – Распределение критического пути (CP_synthesis) в выборке
 Среднее и медианное значения практически совпадают, что свидетельствует о симметричности распределения и отсутствии выраженных выбросов. 
Для наглядности в таблице ниже приведены крайние значения по каждой из ключевых метрик
Таблица 1 – «Анализ конкурентов»
Метрика	Top–3 значения	Bottom–3 значения
LUT	5324, 
5323, 
5233, 
	33, 
33, 
33

DSP	62, 
60,
59
	0, 
0,
0
CP synthesis	12.3, 
11.127, 
11.127
	3.031, 
3.031, 
3.031

CP implementation	12.887, 
10.115, 
10.094
	2.675, 
2.675, 
2.675


Для понимания взаимосвязей между метриками, была построена матрица корреляции, представленная на рисунке 10.
 
Рисунок 10 – Матрица корреляции
Из нее можно сделать вывод, что из-за высокой корреляция между CP_synthesis и CP_implementation использование их одновременно избыточно, так как они несут схожую информацию и нет смысла использовать обе в качестве независимых целевых показателей. Поэтому с данного момента сокращение CP несет в себе смысл как CP_implementation, такой выбор обусловлен тем, что критерий рассчитывается после полного прохождения всех этапов физического проектирования и наиболее приближен к реальным данным.
Умеренная корреляция между LUT и DSP, подтверждает существование конструктивной взаимозависимости между этими типами ресурсов, но также указывает на наличие случаев, где они используются независимо. Корреляция ресурсов с задержками (LUT и CP, DSP и CP) демонстрирует, что модели могут обучаться предсказывать временные характеристики не только по топологии, но и по признакам, отражающим структуру затрат на реализацию.
Более подробная корреляция представлена в приложении.
 
3. РЕАЛИЗАЦИЯ МОДЕЛЕЙ МАШИННОГО ОБУЧЕНИЯ ДЛЯ ЗАДАЧ HLS
3.1. Архитектура используемых графовых нейронных сетей
Для решения задачи предсказания аппаратных метрик в данной работе используются графовые нейронные сети (Graph Neural Networks, GNN). Этот выбор обусловлен несколькими ключевыми факторами. Во–первых, вычислительные графы DFG естественным образом представляют структуру алгоритма, где узлы соответствуют операциям, а рёбра – потокам данных. Во–вторых, GNN способны эффективно обрабатывать графы произвольного размера и структуры, что согласуется, как с собираемыми данными, так и для анализа различных алгоритмов для реальных схем. В–третьих, механизмы агрегации информации в GNN позволяют учитывать как локальные особенности отдельных операций, так и глобальный контекст их взаимодействия. 
В данной работе модели предсказания метрик необходимы для быстрой и точной оценки аппаратных затрат на этапе проектирования цифровых схем. В дельнейшем это будет использовано для быстрой оценки оптимизации RL модели, что поможет ей оценивать распределение ресурсов между различными вариантами оптимизации, не затрачивая время на полный цикл синтеза.
Важно оговорить, что для каждой модели на вход подаётся полный вычислительный граф (полученный ранее), описывающий структуру обрабатываемого алгоритма.
3.2. Модель предсказания LUT
Для задачи предсказания количества LUT в данной работе разработана специализированная архитектура графовой нейронной сети, отражающая вычислительную структуру DFG. 
На первом этапе, для каждого узла графа, применяется линейное преобразование входных признаков с увеличением размерности до 256 и нормализацией с помощью слоя LayerNorm. Это необходимо для перехода от исходного пространства признаков, к более информативному, высокоразмерному скрытому представлению, где модель сможет эффективнее выявлять сложные взаимосвязи между элементами графа. Использование нормализации LayerNorm необходимо для стабильности распределения активаций, это позволит ускорить сходимость обучения и снизить чувствительность модели к вариациям ранее полученные графов имеющих различную структуру и размер.
 В качестве функции активации используется SiLU (Sigmoid Linear Unit), поскольку данная функция показала себя лучше традиционных ReLU и tanh. Далее применяется слой Dropout с вероятностью 0.2 для регуляризации, это необходимо для снижения риска переобучения модели. Dropout случайным образом «отключает» часть нейронов на каждом шаге обучения, что препятствует избыточному запоминанию обучающей выборки. А значение вероятности 0,2 выбрано эмпирически.
 Затем граф проходит через четыре последовательных сверточных блока (ResGCNBlock), каждый из которых реализует два слоя графовой свёртки (GCNConv), слой LayerNorm, функцию активации SiLU и Dropout. Такой выбор глубины архитектуры был подтверждён экспериментально. Увеличение числа блоков не приводило к существенному улучшению качества, а уменьшение снижало точность предсказаний. 
После второго сверточного слоя в каждом блоке применяется механизм внимания (AttentionModule), реализующий self-attention для узлов графа. Это означает, что после того, как признаки узлов были обновлены с учётом информации от соседей с помощью графовой свёртки, дополнительно выполняется операция, позволяющая каждому узлу «взвешенно» агрегировать информацию от всех других узлов в графе. В реализованном AttentionModule для каждого узла формируются три вектора: query, key и value, которые вычисляются с помощью отдельных линейных преобразований. Затем для каждой пары узлов считается скалярное произведение между query одного узла и key другого, нормируется и пропускается через softmax (это способ преобразовать набор чисел в набор вероятностей, которые в сумме дают 1), чтобы получить коэффициенты внимания.
class AttentionModule(nn.Module):
    def __init__(self, in_channels):
        super().__init__()
        self.query = nn.Linear(in_channels, in_channels)
        self.key = nn.Linear(in_channels, in_channels)
        self.value = nn.Linear(in_channels, in_channels)
        self.scale = np.sqrt(in_channels)
        
    def forward(self, x):
        q = self.query(x)
        k = self.key(x)
        v = self.value(x)
        attention = F.softmax(torch.matmul(q, k.transpose(-2, -1)) / self.scale, dim=-1)
        out = torch.matmul(attention, v)
        return out + x  # Residual connection
Размерности выходов блоков последовательно увеличиваются 256 → 512 → 768 → 1024 → 1536, Выбор именно таких размерностей обусловлен эмпирическими экспериментами. После сверточных блоков применяется глобальный self-attention (AttentionModule) размерности 1536. Далее выполняется глобальный средний пуллинг (global_mean_pool), преобразующий признаки всех узлов в один общий вектор фиксированной длины. Это необходимо поскольку в графовой нейронной сети после всех сверточных и attention-слоёв у каждого узла остаётся свой вектор признаков. Однако для предсказания метрики, относящейся ко всему графу необходимо получить единое представление для всего графа, а не для отдельных узлов.
На завершающем этапе используются четыре полносвязных слоя с промежуточными размерностями 1536 → 768 → 384 → 192 → 1. После каждого слоя применяется LayerNorm, функция активации SiLU и Dropout с вероятностью 0.2. Выбор именно таких размерностей связан с исходной размерностью вектора после пуллинга (1536) и необходимостью поэтапно свести её к одному числу. Количество и размерность слоёв подобраны эмпирически. В качестве итогового выхода формируется скаляр, а именно количество LUT для данного графа.
Суммарно архитектура включает:
	4 сверточных блока (ResGCNBlock), каждый из которых содержит по 2 слоя GCNConv
	Механизмы внимания (AttentionModule) после каждого сверточного блока и на этапе глобальной агрегации
	В качестве функции активации на всех этапах используется SiLU
	Регуляризация осуществляется с помощью Dropout (0.2) и LayerNorm
Схематическое представление архитектуры графовой нейронной сети для предсказания количества LUT представлена на рисунке 11. 
 Рисунок 11 – Схематическое представление архитектуры графовой нейронной сети для предсказания количества LUT 
Процесс обучения модели организован с помощью супервизируемого обучения, при которых модели заранее известен правильный ответ. С использованием батчей графов, во время обучения модель обрабатывает не по одному графу за раз, а сразу несколько графов. 
Перед началом обучения целевая переменная масштабируется с помощью стандартизации (StandardScaler), 
x_std=  (x- μ)/σ,				(23)
где μ среднее значение по обучающей выборке, а σ стандартное отклонение.
Это позволяет ускорить сходимость и повысить стабильность процесса оптимизации. 
В качестве функции потерь применяется среднеквадратичная ошибка (MSELoss). Для обновления весов используется алгоритм Adam. 
В работе также реализован механизм ранней остановки, который позволяющий завершать обучение при отсутствии улучшения на валидационной выборке. Данная функция обеспечивает контроль переобучения. И реализуется следующим образом, после каждой эпохи обучения вычисляется значение функции потерь на валидационной выборке. Если в течение 20 последовательных эпох не наблюдается значимого улучшения метрики (в данной работе 〖1e〗^(-6)), процесс обучения автоматически прекращается.
После завершения обучения модель оценивается на тестовой выборке, для которой не использовались данные при обучении и валидации. Для анализа качества предсказаний рассчитываются средняя абсолютная ошибка MAE, средняя квадратическая ошибка MSE, среднеквадратическая ошибка RMSE и коэффициент детерминации (R²). Также проводится визуализация истории обучения, распределения ошибок и сравнение предсказанных и реальных значений аппаратных метрик. 
Наиболее наглядное представление точности модели отражено матрицей ошибок, с количественным диапазоном значении 500, представленный на рисунке 12. 
 
Рисунок 12 – Матрица ошибок предсказаний LUT
 Большинство предсказаний модели попадают в правильный или соседний диапазон, что свидетельствует о высокой точности классификации по диапазонам.
Распределение ошибок предсказания количества LUT на тестовой выборке. Большинство ошибок сосредоточено вблизи нуля, среднее значение ошибки составляет 122,8, стандартное отклонение — 336,7, что указывает на отсутствие систематического смещения и приемлемую точность модели.
 
Рисунок 13 – Распределение ошибок предсказания LUT
Сравнение предсказанных и реальных значений LUT для тестовой выборки представлен на рисунке 14. Точки, расположенные вдоль диагонали, показывают хорошее соответствие между предсказаниями и истинными значениями. Коэффициент детерминации R² = 0.91 свидетельствует о высокой объяснённой дисперсии.
 
Рисунок 14 – Сравнение предсказанных и реальных значений LUT
График изменения функции потерь (loss) на обучающей и валидационной выборках по эпохам. Видно, что модель стабильно обучается, а ранняя остановка предотвращает переобучение.
 
Рисунок 15 – Динамика функции потерь при обучении LUT

В таблице приведены значения основных метрик регрессии, рассчитанных на тестовой выборке для задачи предсказания количества используемых LUT. Эти показатели позволяют оценить точность и надёжность работы модели.
Таблица 1 – «Анализ конкурентов»
Метрика	Значение
MAE	267,55
MSE	128474,20
RMSE	358,43
R²	0,91

Модель предсказания критического пути
Второй была разработана модель для оценки критического пути схемы. Несмотря на общность базовой инфраструктуры обработки графов, архитектура модели CP имеет ряд существенных отличий, обусловленных спецификой распределения CP и экспериментами. В процессе исследования улучшения модели CP путем применения более сложной архитектуры, аналогичной модели LUT, с расширенной последовательностью ResGCN блоков и механизмом внимания. Однако эксперименты показали, что усложнение архитектуры приводит к ухудшению качества предсказаний критического пути. Поэтому была выбрана более компактная структура. 
Рассмотрим основные различия:
	LayerNorm не используется.
	Функция активации ReLU.
	Нет отдельных блоков. Просто два последовательных GCNConv
	Отсутствуют механизмы внимания.
	Размерности слоев input_dim → 128 → 128.
	Завершающие слои: 2 полносвязных слоя (128 → 64 → 1) с ReLU и Dropout.
Но данная модель оказалась более чувствительна к гиперпараметрам обучения, она требует большее количество эпох 300, в сравнении со 150 для LUT, для достижения оптимальных результатов, это можно объяснить использованием ReLU и малым диапазоном распределения временных характеристик. Из-за чего модель медленнее сходится.
Итоговая архитектура модели отображена на рисунке.
 
Рисунок
Матрица ошибок для диапазонов DSP, представленная на рисунке 16, указывает, что большинство предсказаний попадает в правильный или соседний диапазон, что говорит о высокой точности модели при оценке использования DSP.
 
Рисунок 16 – Матрица ошибок предсказаний CP
Распределение ошибок предсказания CP представленная на рисунке 17 отражает, что основная масса ошибок сосредоточена вблизи нуля, а среднее значение ошибки близко к нулю. Это указывает на отсутствие систематического смещения и подтверждает хорошую точность модели при оценке критического пути.
 
Рисунок 17 – Распределение ошибок предсказания CP
Диаграмма рассеяния предсказанных и реальных значений CP на рисунке 18 демонстрирует, что точки в основном располагаются вдоль диагонали, что говорит о высоком соответствии между предсказаниями и истинными значениями. Коэффициент детерминации R² дополнительно подтверждает качество аппроксимации.
 
Рисунок 18 – Сравнение предсказанных и реальных значений CP
График изменения функции потерь указанный на рисунке 19 на обучающей и валидационной выборках показывает стабильное обучение модели. Небольшое расхождение между кривыми свидетельствует об отсутствии переобучения и устойчивости процесса оптимизации.
 
Рисунок 19 – Динамика функции потерь при обучении CP
В таблице приведены значения метрик регрессии, рассчитанных на тестовой выборке для задачи предсказания значения критического пути. Эти показатели отражают способность модели точно аппроксимировать целевую метрику.
Таблица 1 – «Анализ конкурентов»
Метрика	Значение
MAE	0,51
MSE	0,49
RMSE	0,70
R²	0,72

Модель предсказания DSP
Для предсказания использования DSP–блоков была разработана модель, которая, как и предыдущие модели (LUT и CP), основана на графовых нейронных сетях. Базовая инфраструктура обработки графов остается общей для всех моделей. Но стоит уточнить, что в ходе работы рассматривались различные варианты архитектуры, включая глубокие модели с attention-модулями, аналогичные LUT-модели. Однако эксперименты показали, что для задачи предсказания количества DSP наилучшие результаты достигаются при использовании более компактной и специализированной структуры, что может быть также объяснено распределением значений.
Основные отличия архитектуры модели DSP:
	LayerNorm не используется.
	В качестве функции активации применяется ReLU.
	Вместо глубоких ResGCN-блоков используются два слоя GCNConv с увеличенной размерностью скрытых слоёв (input_dim → 256, 256 → 128).
	После каждого сверточного слоя применяется BatchNorm1d.
	Механизмы внимания отсутствуют.
	Завершающие слои 128 → 256 → 128 → 1.
	Dropout применяется с вероятностью 0.3 для усиления регуляризации.
Итоговая архитектура модели отображена на рисунке
 
Разберем подробнее BatchNorm1d, он применяется для ускорения стабилизации модели. Этот слой используется после каждого сверточного и полносвязного слоя, осуществляя нормализацию выходов предыдущего слоя по каждому признаку отдельно, приводя их к нулевому среднему и единичному стандартному отклонению. Для этого внутри каждого мини-батча вычисляется среднее значение и стандартное отклонение для каждого признака. Затем из каждого значения этого признака вычитается среднее и результат делится на стандартное отклонение. В итоге после этого среднее значение признака по батчу становится равным нулю, а разброс значений единице.
self.conv1 = GCNConv(input_dim, hidden_dim * 2)
self.bn1 = nn.BatchNorm1d(hidden_dim * 2)
self.conv2 = GCNConv(hidden_dim * 2, hidden_dim)
self.bn2 = nn.BatchNorm1d(hidden_dim)

def forward(self, x, edge_index, batch):
    x = F.relu(self.bn1(self.conv1(x, edge_index)))
    x = self.dropout(x)
    x = F.relu(self.bn2(self.conv2(x, edge_index)))
    x = self.dropout(x)
    # и далее по архитектуре ...
Матрица ошибок для диапазонов DSP, представленная на рисунке 20, показывает, что большинство предсказаний модели попадает в правильный или соседний диапазон. Это свидетельствует о высокой точности классификации значений DSP и способности модели корректно определять диапазон использования цифровых сигнальных процессоров для большинства графов.
 
Рисунок 20 – Матрица ошибок предсказаний DSP
Распределение ошибок предсказания DSP на рисунке 21 отражает, что основная масса ошибок сосредоточена вблизи нуля, а среднее значение ошибки составляет 1.13. Это указывает на отсутствие выраженного смещения и подтверждает хорошую точность модели при оценке использования DSP.
 
Рисунок 21 – Распределение ошибок предсказания DSP
Диаграмма рассеяния предсказанных и реальных значений DSP, представленная на рисунке 22, демонстрирует, что точки в основном располагаются вдоль диагонали, что говорит о высоком соответствии между предсказаниями и истинными значениями. Коэффициент детерминации R² = 0.84 дополнительно подтверждает качество аппроксимации.
 
Рисунок 22 – Сравнение предсказанных и реальных значений DSP
График изменения функции потерь на обучающей и валидационной выборках для модели DSP, приведённый на рисунке 23, показывает стабильное обучение модели. Небольшое расхождение между кривыми свидетельствует об отсутствии переобучения и устойчивости процесса оптимизации.
 
Рисунок 23 – Динамика функции потерь при обучении DSP
В таблице представлены значения метрик регрессии, полученные на тестовой выборке для задачи предсказания количества используемых DSP-блоков. Данные метрики характеризуют точность и стабильность предсказаний модели.
Таблица 1 – «Анализ конкурентов»
Метрика	Значение
MAE	2,60
MSE	15,73
RMSE	3,9663 
R²	0,84

3.2. Система обучения с подкреплением
В рамках данной работы разработана система обучения с подкреплением для оптимизации распределения аппаратных ресурсов, основанная на алгоритме Actor–Critic. Данный подход обеспечивает возможность последовательного принятия решений о способе реализации операций умножения, учитывая комплексное влияние каждого решения выбора аппаратных метрик на характеристики схемы.
	Моделирование среды синтеза схемы
	Инициализация среды
В рамках реализации среды обучения с подкреплением для оптимизации распределения аппаратных ресурсов инициализация среды осуществляется в конструкторе класса RLEnv. На данном этапе производится подготовка всех необходимых компонентов для корректного взаимодействия агента с моделью синтеза.
В первую очередь определяется вычислительное устройство, на котором будут выполняться все операции, GPU если доступно или CPU. Далее производим загрузку предобученных нейросетевых моделей, которые будут оценивать аппаратные метрики LUT, DSP и CP. Каждая из моделей инициализируется с заданными параметрами входного и скрытого слоёв, после чего на неё загружаются веса, полученные в результате предварительного обучения. Также для корректной работы с выходами моделе загружаются соответствующие объекты масштабирования (scaler), необходимые для обратного преобразования нормализованных значений метрик к исходному масштабу.
После подготовки моделей среда переводит их в режим оценки, .eval(). Данная операция отключает механизмы, характерные для стадии обучения, обновления параметров, вычисление градиентов и стохастические компоненты (Dropout, BatchNorm). В результате параметры моделей фиксируются, и их значения остаются неизменными на протяжении всего взаимодействия с агентом, что позволяет гарантировать отсутствие обновления весов в процессе взаимодействия с агентом. 
Далее осуществляется загрузка набора графов, полученных на стадии подготовки из graph_dataset.pkl, а также соответствующих целевых значений аппаратных метрик DSP. Эти данные позволяют обучить модель и оценить ее работу на дальнейших этапах. 
В завершение определяются внутренние переменные среды, отвечающие за текущее состояние, прогресс по эпизоду, а также за отслеживание операций умножения и их распределения между различными типами реализации. Таким образом, для каждого графа определяется список индексов узлов, соответствующих операциям умножения, для всех них изменяется параметр, отвечающий за аппаратную реализацию, устанавливается как используемый LUT (f10 = 10). Это необходимо чтобы Агент учился понимать, где использовать DSP (f10 = 0). Не подсматривая готового решения. Для отслеживания прогрессу по эпизоду разработана функция, рассчитывающаяся как отношение текущего номера обрабатываемой операции к общему числу операций умножения в графе. Этот показатель включается в вектор состояния, предоставляя агенту информацию о текущем этапе процесса распределения ресурсов, позволяя оценить как далеко агент от начала и как близок к концу. 
	Сброс среды
Сброс среды осуществляется благодаря использованию метода reset, который подготавливает все компоненты среды к началу нового эпизода. На данном этапе выбирается целевой граф, копируются его признаки узлов и рёбер, а также устанавливается соответствующее целевое значение аппаратной метрики DSP. Для всех операций умножения в графе устанавливается тип реализации на LUT.
В процессе сброса формируется список индексов узлов, соответствующих операциям умножения, и определяется максимальное количество шагов в эпизоде, соответствующее числу таких операций. Для каждой операции инициализируется структура, проверяется отсутствие назначенного типа реализации на начальном этапе. После этого обнуляются счётчики прогресса и статистики, связанные с количеством операций умножения и числом назначенных DSP.
После выполнения всех подготовительных действий формируется вектор состояния, отражающий текущее распределение признаков по узлам, положение в последовательности операций, а также целевое значение аппаратной метрики. Этот вектор отправляется агенту агенту для принятия решений в новом эпизоде, являясь начальной точкой среды исполнения. Это позволяет обеспечить воспроизводимость эксперимента и корректную инициализацию всех параметров.
	Выполнение действий
Выполнение действия в среде реализуется с помощью метода step, он служит для обработки одного шага взаимодействия агента с системой. На каждом шаге определяется текущая операция умножения, подлежащая распределению. Агент выбирает действие, соответствующее назначению типа реализации для данной операции либо использовать DSP, либо реализовать через LUT. В зависимости от выбранного действия в структуре признаков узлов графа обновляется соответствующий атрибут, отражающий тип реализации для текущей операции f10.
После применения действия происходит переход к следующей операции умножения, что сопровождается обновлением внутреннего состояния среды, ее индексы и статистика распределения ресурсов. На основании обновлённых признаков формируется новое состояние, которое включает в себя актуальную информацию о распределении операций, прогрессе по эпизоду и целевых метриках. Это состояние возвращается агенту для принятия последующих решений. Это обеспечивает поэтапное и контролируемое распределение аппаратных ресурсов, что позволяет агенту учитывать последствия каждого отдельного действия в контексте всей схемы.
3.3.4. Формирование состояния 
Формирование состояния среды осуществляется в методе _get_state, он преобразует текущую конфигурацию графа в вектор признаков, пригодный для подачи на вход агенту. 
На первом этапе признаки узлов и рёбер графа конвертируются в числовой формат, соответствующий требованиям предобученных моделей оценки аппаратных метрик. Для этого производится извлечение и структурирование необходимых признаков, а также формирование индексов связей между узлами.
Далее, с использованием предобученных нейросетевых моделей, для текущего состояния графа вычисляются эмбеддинги, отражающие его аппаратные характеристики по трём ключевым метрикам: LUT, DSP и CP. Полученные эмбеддинги усредняются по всем узлам графа, что позволяет получить компактное и информативное представление структуры схемы.
На завершающем этапе формируется итоговый вектор состояния, который включает в себя полученные эмбеддинги, нормализованное целевое значение аппаратной метрики DSP, а также показатель прогресса по эпизоду, отражающий долю уже обработанных операций умножения. Такой вектор состояния содержит всю необходимую информацию для принятия агентом обоснованных решений на каждом шаге эпизода.
3.3.5. Вычисление награды (_calculate_reward)
Вычисление награды в среде реализуется в методе _calculate_reward, он обеспечивает количественную оценку качества решений, принимаемых агентом по завершению каждого эпизода. 
На первом этапе с помощью предобученных моделей производится оценка текущих значений аппаратных метрик LUT, DSP и CP для сформированной конфигурации графа. Полученные значения преобразуются из нормализованного формата, из GNN, в исходный масштаб. Это делается для того, чтобы разные метрики, которые изначально имеют разный порядок величин, были приведены к сопоставимому масштабу. 
Далее необходимо привести каждую метрику к масштабу, где среднее значение равно нулю, а стандартное отклонение единица, это возможно благодаря ранее рассчитанным распределениям метрик. Это необходимо поскольку позволяет сделать вклад каждой метрики в итоговую награду сопоставимым
norm_lut = (current_lut - self.lut_mean) / self.lut_std
norm_cp = (current_cp - self.cp_mean) / self.cp_std
Нормализированное значение=  (Исходное значение-Среднее)/(Стандартное отклонение)		  (24)
В результате после такого преобразования:
	Если значение совпадает со средним, результат будет 0.
	Если значение больше среднего на одно стандартное отклонение, результат будет 1. Если меньше среднего на одно стандартное отклонение, результат будет -1.
Далее рассчитывается сама награда, она учитывает отклонение текущего значения DSP от целевого, а также значения LUT и CP. И рассчитывается для всего графа только в конце эпизода. Для данной работы, наиболее продуктивные результаты оптимизации показала награда, рассчитанная следующим образом:
 reward = -self.alpha * norm_lut - self.lambda0 * norm_cp
        DSP_REWARD_SCALE = 6  # Усиление награды за DSP

        # Штраф за ошибку по количеству DSP относительно целевого значения
        dsp_error = abs(current_dsp - self.target_dsp_value) / self.dsp_std
        reward -= (1 - self.alpha) * dsp_error * DSP_REWARD_SCALE

        # Большой бонус за точное попадание в целевой DSP (±2.6)
        if abs(current_dsp - self.target_dsp_value) <= 2.6:
            reward += 5.0 * DSP_REWARD_SCALE
        elif abs(current_dsp - self.target_dsp_value) <= 4:
            reward += 2.0 * DSP_REWARD_SCALE
            # Маленький бонус за почти точное попадание (±5)
            #elif abs(current_dsp - self.target_dsp_value) <= 5.2:
            #    reward += 2.0 * DSP_REWARD_SCALE
Здесь, в первую очередь, базовая часть награды формируется как взвешенная сумма нормализованных значений аппаратных метрик LUT и CP с отрицательным знаком. Это означает, что агенту выгодно минимизировать оба показателя – чем меньше значения LUT и CP, тем выше итоговая награда. Веса α, λ позволяют регулировать относительную важность каждой из метрик в общей формуле (в данной работе оба значения 0,3).
Далее вводится дополнительный штраф за отклонение количества используемых DSP от целевого значения. Для этого вычисляется абсолютная ошибка между предсказанным и целевым количеством DSP, нормализованная на стандартное отклонение. Полученное значение умножается на масштабирующий коэффициент и дополнительный вес (1−α), такой выбор параметр обусловлен балансом между двумя целями LUT и DSP, и подобран в ходе экспериментов. 
В завершение, если агенту удаётся попасть в целевое значение DSP с высокой точностью разница не превышает 2,6, что соответствует RMSE для модели GNN для DSP, он получает значительный бонус к награде. Если отклонение чуть больше до 4, начисляется меньший бонус. Эти поощрения стимулируют агента к поиску решений, максимально близких к целевому количеству DSP.
3.3.7. Архитектура модели ActorCritic
Для начала подробнее рассмотрим гиперпараметры. Класс ActorCritic определён как наследник базового класса nn.Module из библиотеки PyTorch. Данное решения принято по причине общей реализации с помощью библиотеки torch, что позволяет автоматически вычислять градиенты, хранить параметры и быть совместимой с остальными инструментами данной библиотеки.
Параметр input_dim=633 определяет размер входного вектора признаков, который подаётся на вход модели. Значение 633 выбрано исходя из структуры состояния среды:
	210 признаков для LUT-эмбеддинга. 
Что соответствует максимальному количеству узлов для используемого в работе наборе данных. 
	210 признаков для DSP-эмбеддинга.
	210 признаков для CP-эмбеддинга.
	1 признак нормализованное целевое значение DSP.
	1 признак прогресс по эпизоду. 
На каком этапе распределения операций умножения находится агент в текущем эпизоде.
	1 дополнительный признак.
В данной работе он не влияет на работу модели и всегда равен 1.
Параметр hidden_dim=256 определяет размер скрытого слоя в общей части нейронной сети. Это значение выбрано экспериментально, оно не приводит к избыточному увеличению числа параметров и времени обучения. Если потребуется, размер скрытого слоя можно изменить для адаптации под другие задачи или объёмы данных.
Структура модели включает три основные компонента. В качестве общего энкодера используется последовательность из трёх полносвязных слоёв. После первых двух слоёв применяются функции активации LeakyReLU. После третьего слоя используется функция активации Tanh, такой выбор был сделан на основе экспериментальных результатов, как наиболее подходящий.
Далее, выход энкодера подаётся на два отдельных модуля.
Модуль actor реализован в виде одного полносвязного слоя, который принимает на вход выходной вектор из энкодера. Этот слой преобразует внутреннее представление состояния в вектор, размерность которого соответствует числу возможных действий агента (в данной задаче два, назначить DSP или LUT для текущей операции умножения). На выход этого слоя подаётся функция активации Softmax. Softmax преобразует полученные значения в вероятности таким образом, что каждое значение становится числом от 0 до 1 и сумма их сумма равна 1. Получается что если на выходе actor-модуля получено [0.8,0.2], с вероятностью 80% на операцию умножения назначается действие DSP, а с вероятностью 20% LUT.
Модуль critic также реализован как один полносвязный слой, который принимает на вход выход энкодера и преобразует его в одно скалярное значение. Это значение интерпретируется как оценка ценности (value) текущего состояния среды, то есть насколько выгодно, с точки зрения долгосрочной награды, находиться в данном состоянии.
В отличие от actor-модуля, который формирует вероятности для выбора действия, critic-модуль не использует функцию активации на выходе и возвращает просто число. Это число показывает, насколько выгодно, по мнению модели, находиться в данном состоянии с точки зрения ожидаемой суммарной награды в будущем. То есть, если critic возвращает 5.2, то модель считает, что из этого состояния можно получить в среднем 5.2 награды до конца эпизода. Это используется для вычисления ошибки обучения и корректировки стратегии агента. Позволяя ему отличать хорошие состояния от менее выгодных.
3.3.8. Система обучения модели Actor–Critic
Процесс обучения организован в виде последовательных эпизодов, в ходе которых агент взаимодействует с моделируемой средой, накапливает опыт и постепенно улучшает свою стратегию на основе получаемых наград. 
На этапе инициализации обучения задаются основные параметры процесса. Количество эпизодов, скорость обучения (параметр, который определяет, насколько сильно обновляются веса нейронной сети на каждом шаге оптимизации во время обучения, в данной работе он выбран 0.001 экспериментально), коэффициенты дисконтирования (параметр, который определяет, насколько сильно агент учитывает будущие награды по сравнению с текущей γ близко к 1, агент старается максимизировать суммарную награду в долгосрочной перспективе, в противном случае, агент больше ориентируется на ближайшие награды, для данной работы 0.9 выбрано эксперементально, отражая что будущие награды почти так же важны, как и текущие) и веса для различных компонентов функции награды. Далее определяется вычислительное устройство GPU, если оно доступно, иначе CPU. После этого создаётся экземпляр среды, моделирующей процесс распределения аппаратных ресурсов, и инициализируется агент на основе архитектуры Actor–Critic. И загружается набора графов, которые будут использоваться в процессе обучения. 
Для отслеживания процесса обучения модели в рамках данной системы реализован комплексный мониторинг ключевых метрик, отражающих динамику и качество принимаемых агентом решений. В ходе каждого эпизода фиксируются значения награды, аппаратных метрик с помощью моделей GNN и показатели точности попадания в целевое значение DSP, а также эффективность использования DSP для отслеживания что модель не назначает все умножения на DSP и одновременно ими не пренебрегает ими. Эти данные накапливаются в процессе обучения и используются для построения скользящих средних, в течение 100 эпизодов, что позволяет выявлять тенденции и оценивать устойчивость стратегии агента.
Собранная статистика визуализируется с помощью графиков, отображающих изменение награды, распределение аппаратных ресурсов и точность по целевым метрикам. Кроме того, предусмотрена возможность сохранения метрик в табличном виде для последующего анализа. Такой подход обеспечивает прозрачность процесса обучения, позволяя оперативно выявлять проблемы, связанные с переобучением или стагнацией, и принимать обоснованные решения о необходимости корректировки параметров модели или среды.
В системе также реализованы механизмы ранней остановки обучения на основе анализа динамики средней награды, что позволяет предотвратить избыточное обучение. Если наблюдается резкий рост средней награды между двумя соседними окнами, обучение автоматически завершается. Такой критерий позволяет зафиксировать момент, когда агент совершил качественный скачок в обучении, и дальнейшее продолжение процесса становится нецелесообразным. В данной работе это выполнено для проведения экспериментов, при котором неверные настройки параметров моделей и награды могли приводить к резкому переобучению моделей. Финальный вариант работы, а именно настроенные в ней параметры, не испытывает проблем с переобучением и соответственно не вызывает раннюю остановку.
В основном цикле обучения для каждого эпизода выполняется последовательность шагов, обеспечивающих накопление опыта агентом и обновление параметров модели. На первом этапе случайным образом выбирается граф из набора данных и соответствующее ему целевое значение DSP, после чего происходит сброс среды. В ходе эпизода агент пошагово взаимодействует со средой, на каждом шаге вычисляются вероятности возможных действий и оценка состояния с помощью модели, выбирается действие, которое затем применяется в среде. Результаты каждого шага (состояние, действие, ценность) сохраняются для последующего обучения. После выполнения действия агент переходит к следующему состоянию, и процесс повторяется до завершения эпизода, когда обработаны все операции умножения, после чего рассчитывается награда, характеризующая результат оптимизации графа. По окончании эпизода накопленные данные используются для визуализации метрик мониторинга, что позволяет отслеживать динамику обучения и эффективность стратегии агента.
Важно сказать, что модель считается обученной, если средняя награда в течении окна наблюдения выходит на плато. Плато считается диапазон значений средней награды, в котором её колебания не превышают определенный порог, в данной работе 0,04. Поэтому для сохранение лучшей модели реализовано два подхода. Первый, сохраняет модель при достижении нового максимального значения средней награды за выбранное окно эпизодов. Второй, сохраняет модель при достижении нового лучшего комбинированного показателя (учитывающего точность DSP, LUT и CP). Комбинированный показатель рассчитывается по формуле:
combo_score=dsp_accuracy-0.0001×mean_lut-0.01×mean_cp     (25)
На практике оба варианта сохранения лучшей модели почти всегда срабатывали в одинаковый момент.
Для комплексной оценки процесса обучения и анализа поведения агента в ходе оптимизации аппаратных метрик в работе реализована система мониторинга, результаты которой представлены в виде графиков далее. На графиках отражены основные показатели, позволяющие отслеживать прогресс обучения, выявлять тенденции и своевременно обнаруживать возможные проблемы в стратегии агента.
Лучшая метрика достигается на 554 эпизоде, после чего средняя награда выходит на плато в диапазоне [0,4 – 0,44].
График на рисунке 24 отображает изменение награды за каждый эпизод и скользящее среднее значение награды. Рост средней награды свидетельствует о прогрессе агента в обучении и улучшении его стратегии.
 
Рисунок 24 – Динамика награды агента
График на рисунке 25 иллюстрирует изменение количества используемых DSP по мере обучения. Видно, что агент постепенно учится более экономно распределять ресурсы. На графиках отчётливо прослеживается тренд на экономию DSP, по мере обучения агент всё реже назначает операции умножения на специализированные блоки, что приводит к снижению общего количества используемых DSP.

 
Рисунок 25 – Динамика использования DSP
График на рисунке 26 демонстрирует динамику критического пути для каждого эпизода. Значения остаются в характерном диапазоне, что подтверждает стабильность работы модели по данной метрике. наблюдается тенденция к экономии по CP, что выражается в уменьшении значений этой метрики на протяжении обучения.
 
Рисунок 26 – Динамика критического пути

График на рисунке 27 показывает динамику предсказанных значений LUT для каждого эпизода. Разброс значений отражает разнообразие графов и решений, принимаемых агентом. обратный тренд — увеличение количества используемых LUT, что связано с перераспределением операций и необходимостью компенсировать снижение других ресурсов за счёт LUT.
 
Рисунок 27 – Динамика использования LUT
График на рисунке 28 отражает долю эпизодов, в которых агенту удалось достичь целевого значения DSP с заданной точностью. Рост этого показателя указывает на повышение эффективности стратегии распределения ресурсов.
 
Рисунок 28 – Точность достижения целевого значения DSP
График на рисунке 29 иллюстрирует отношение числа назначенных DSP к общему количеству операций умножения. Снижение этого показателя свидетельствует о том, что агент учится использовать DSP только там, где это действительно необходимо.
 
Рисунок 29 – Эффективность использования DSP
График на рисунке 30 иллюстрирует что модель старается оптимизировать значения, а не выбирает политику неиспользования DSP.

 
 
Рисунок 30 – Доля эпизодов с нулевым использованием DSP
На рисунке 31 график сравнивает общее количество операций умножения и число назначенных DSP в каждом эпизоде. По мере обучения наблюдается сокращение числа назначенных DSP при сохранении или даже увеличении общего количества операций, что подтверждает тренд на более рациональное распределение ресурсов.
 
Рисунок 31 – Динамика количества операций умножения и назначенных DSP
4. ЭКСПЕРИМЕНТАЛЬНЫЕ ИССЛЕДОВАНИЯ
В процессе тестирования обученной модели на простых примерах, характеризующихся однородной размерностью узлов, отсутствием множественных выходов из одного узла, а также выполнением операций умножения преимущественно в начале или в конце алгоритма, было отмечено следующее. Модель демонстрирует тенденцию к поиску компромисса между экономией различных аппаратных ресурсов, но не может ее корректно достичь, поскольку в выборке отсутствовали малые графы.
В рамках исследования были проведены эксперименты для ряда типовых алгоритмов цифровой обработки сигналов, быстрое преобразование Фурье (FFT), FIR- и IIR-фильтры. Рассмотрим подробнее на примере FIR-фильтра, алгоритм выполнения которого представлен на рисунке. 
 
В ходе оптимизации модель демонстрировала неустойчивое поведение по ряду метрик, могли возникать отрицательные значения LUT или значительные отклонения по критическому пути. 
Пример результата оптимизации FIR-фильтра
Лучшее распределение узлов (итерация 5):
	LUT узлы: [16, 22, 23]
	DSP узлы: [17, 18, 19, 20, 21]
Метрики лучшего распределения:
	CP: 8.79 (целевое: 3.88)
	DSP: 3.54 (целевое: 8.00)
	LUT: -1240.28 (целевое: 34.00)
После синтеза с помощью Vitis HLS получаем значения метрик представленные в таблице
Таблица
Параметр	Оптимизация Vitis HLS без указания директив	Оптимизация Vitis HLS с указанием директив
DSP	8	5
LUT	34	37
CP	3.884	4.077
Таким образом, проведение оценки модели на подобных простых примерах не имеет практического смысла по двум основным причинам. Во-первых, отсутствует возможность получить адекватные предсказания для аппаратных метрик. Модель может выдавать аномальные или физически невозможные значения, а реальные значения после синтеза оказываются жёстко зафиксированными инструментом Vitis HLS. Во-вторых, из-за простоты самого алгоритма синтезатор назначает все операции умножения исключительно на DSP-блоки, а операции сложения на LUT, что видно из приведённой выше таблицы. Это связано с внутренней логикой работы Vitis HLS, при синтезе простых или небольших графов инструмент стремится к максимально эффективному и однозначному распределению ресурсов, минимизируя задержки. 
В более сложных и крупных графах возможны ситуации, когда для одной операции может быть назначено несколько LUT или DSP-блоков, например, при реализации широких или многоразрядных операций, а также при оптимизациях, связанных с параллелизмом или особенностями архитектуры целевого устройства. Однако для простых примеров, рассмотренных в данном исследовании, подобные случаи не наблюдаются, и распределение ресурсов остаётся тривиальным и однозначным. 
В качестве практической задачи предлагается использовать сгенерированные тестовые графы, имитирующие структуру криптографических хэш-функций. Для таких графов характерна высокая сложность, наличие нелинейных зависимостей и большое количество операций, что делает их более релевантными для оценки возможностей разработанной системы оптимизации. 
Для таких графов, алгоритм, показывает, что разработанная система оптимизации, отображает экономию по всем ключевым аппаратным метрикам по сравнению с эталонными решениями, разберем на примере трех случаев, представленных на рисунках.

 
рисунок
В данном случае оптимизация приводит к экономии по CP и DSP, которые составляют примерно 93% и 85% от целевых значений соответственно, при этом LUT превышает целевое. Это может свидетельствовать о том, что для данного графа удалось достичь баланса между сокращением DSP и CP, однако для минимизации этих метрик потребовалось увеличить использование LUT.
 
Здесь наблюдается обратная ситуация, по метрике DSP оптимизированное решение превышает целевое значение, достигая около 137%, тогда как по CP и LUT достигается экономия, примерно 96% и 92% от целевого соответственно. Это демонстрирует, что в некоторых случаях оптимизация по одной метрике может сопровождаться увеличением затрат по другой, что отражает наличие компромиссов в процессе распределения ресурсов.
 
На этом примере видно, что по сравнению с целевыми значениями оптимизированное решение достигает значительной экономии по всем метрикам, DSP около 38% от целевого и LUT около 55% от целевого. Значение CP также снижено, но остаётся ближе к целевому уровню около 90%. Это указывает на эффективное сокращение использования вычислительных ресурсов при сохранении приемлемого значения критического пути.
В рамках данного тестирования анализировались результаты работы модели на 20 различных тестовых графах. Для оценки эффективности оптимизационной модели был построен сводный график, представленный на рисунке.
 
Данный график отражает средние значения дельты между результатами, полученными с помощью оптимизационного алгоритма, и полученными в ходе синтеза Vitis HLS, для основных аппаратных метрик для всех 20 тестовых примеров.
Дельта рассчитывалась как разность между значением метрики, полученной после оптимизации, и целевым значением, нормированная на целевое значение и выраженная в процентах. Отрицательные значения зелёные столбцы соответствуют экономии ресурса относительно эталона, то есть оптимизационный алгоритм показал лучшие результаты по сравнению с базовым решением.
Таким образом, график наглядно демонстрирует, что в среднем по всем тестовым графам достигается экономия по всем ключевым метрикам, что подтверждает эффективность разработанной системы оптимизации. Это подтверждает, что предложенный подход не только применим к задачам цифрового проектирования, но и обладает потенциалом для генерации более эффективных решений. 
Дальнейшее увеличение выборки тестовых графов, а также дополнение её примерами с другими типами операций, такими как пофазовый сдвиг и другие характерные для цифровых алгоритмов преобразования. Может позволить более полно оценивать универсальность и адаптивность разработанной системы оптимизации в различных сценариях цифрового проектирования.
Не исключено, что использование альтернативных архитектур агента или более сложных моделей может привести к ещё более точным результатам. Однако такие эксперименты требуют значительных вычислительных ресурсов и времени. В рамках данной работы основной целью являлась демонстрация работоспособности предложенного подхода и его применимости к задачам оптимизации аппаратных метрик на сложных графах. Полученные результаты подтверждают, что выбранная стратегия обучения с подкреплением способна эффективно решать поставленную задачу и обеспечивает экономию ресурсов по всем ключевым метрикам.
 
5. СОСТАВЛЕНИЕ БИЗНЕС-ПЛАНА ПО КОММЕРЦИАЛИЗАЦИИ РЕЗУЛЬТАТОВ НИР
5.1. Описание проекта
5.1.1. Резюме
Данная разработка позволит инженерам создавать более эффективные цифровые схемы, затрачивая на это меньше времени. Данный продукт использует графовые нейронные сети для точного предсказания метрик (LUT, DSP, CP), а также содержит алгоритм обучения с подкреплением для нахождения оптимального баланса этих показателей, сокращая время и экономя ресурсы, требуемые для получения оптимального результата, что также позволяет минимизировать зависимость готовой архитектуры от опыта разработчика. 
В разработки необходима команда из минимум 4 специалистов, обладающих компетенциями в области машинного обучения, обработки данных и сопровождения инфраструктуры ML, EDA, DevOps, QA. Для аппаратные ресурсы предполагается развернуть свою серверную инфраструктуру, для обучения и сбора метрик, программное обеспечение будет базироваться на среде разработки Vitis и интегрированной среде PyCharm.
Целевая аудитория — это компании, занимающиеся разработкой цифровых схем. Из-за малого количества крупных компаний, заинтересованных в данной разработке, был выбран курс на малый и средний бизнес, с возможностью приобрести продукт, как услугу, на определенный срок (1 мес.). С возможностью дальнейшего продления.
Ожидается, что к концу второго года реализации проекта выручка составит не менее 4 млн рублей. Срок окупаемости проекта составляет два года. NPV превышает 2,7 млн рублей, а внутренняя норма доходности IRR составляет 42 %, что свидетельствует о высокой инвестиционной привлекательности проекта.
5.1.2. Описание продукции
Продукт: Интеллектуальная система автоматизированной оптимизации параметров высокоуровневого синтеза.
Назначение данной разработки заключается в повышении эффективности проектирования цифровых устройств путём снижения временных затрат на синтез. Система реализует два взаимосвязанных функциональных модуля, модуль предсказания характеристик схем и модуль оптимизации параметров. Архитектура решения обеспечивает совместимость с существующими инструментами HLS, в частности Vitis HLS.
С технической точки зрения продукт реализует анализ графа потока данных, извлечённого из исходного алгоритмического описания схемы, и формирование признакового векторного представления узлов с последующей передачей этой информации в оптимизатор. На основе результатов предсказания аппаратных метрик для исследуемого запускается оптимизационный цикл, в рамках которого RL-агент определяет оптимальную конфигурацию параметров синтеза. 
По сравнению с аналогами (Xilinx Vitis и Intel Quartus) в предлагаемом продукте реализована интеллектуальная система подбора параметров на основе анализа структуры проекта. 
В рамках реализации проекта планируется выпуск программного модуля в виде лицензионного продукта с возможностью расширения. Базовый функционал включает модель предсказания метрик, модуль оптимизации параметров, а также компоненты визуализации результатов до 10000 LUT. Полная версия программы не имеет ограничений и гарантирует приоритетную поддержку. На момент составления бизнес-плана продукт находится на стадии завершения прикладных исследований.  Условия поставки предполагают гибкую модель лицензирования с доступом к обновлениям и сопровождением в течение всего срока эксплуатации.
5.1.3. Анализ рынка сбыта
Данная система ориентирована на рынок программных средств автоматизированного проектирования цифровых устройств, сегмент FPGA решений. Потенциальными потребителями продукции являются компании, специализирующиеся на разработке аппаратных решений, исследовательские центры, а также стартапы, занимающиеся созданием аппаратных ускорителей и обработки сигналов. 
Рынок программных решений для автоматизации синтеза находится на стадии активного роста, что обусловлено повышением сложности современных цифровых систем. Основные сегменты данного рынка — это разработчики нейропроцессоров и IoT-устройств. Наибольший коммерческий потенциал демонстрируют малые и средние проектные команды, сталкивающиеся с необходимостью оптимизации ресурсов проектирования, но не обладающие достаточными кадровыми или вычислительными мощностями для самостоятельного проведения анализа. Продукт является надстройкой над существующими системами HLS.
Объём рынка мирового рынка цифровых устройств оценён $393,63 млрд рублей в год, с долей малых и средних компаний до 30%. В среднем 40–50 организаций ежегодно запускают новые проекты, требующие инструментов проектирования аппаратных ускорителей [1]. Потенциальная доля Российского рынка при успешном внедрении инновационного продукта может составить 10–15% в течение первых двух лет. При базовой стоимости программной лицензии с сопровождением на уровне 5 тыс. руб./мес. и 15 тыс. руб./мес. за продвинутую объём продаж может достичь 3 млн рублей в первые два года с дальнейшим постепенным ростом. 
5.1.4. Анализ конкурентов
Рынок программных решений в области автоматизированного высокоуровневого синтеза характеризуется высокой степенью специализации и ограниченным числом игроков. Ключевыми конкурентами являются крупные вендоры средств разработки цифровых схем. (Где ссылка на таблицу 5.1?)
Таблица 5.1 – «Анализ конкурентов»
Конкуренты	Преимущества	Недостатки	Маркетинг	Стратегия
Xilinx	Интеграция с Vitis, гибкие директивы	Нет оптимизации подбора параметров, ограниченная предсказуемость	Прямые продажи, обучение	Закрытая экосистема
Intel	Поддержка OpenCL, совместимость с Quartus	Нет оптимизации подбора параметров, ложность настройки	Интеграция в платформу	Привязка к оборудованию
По проведенному анализу – рынок ориентирован на решения крупных производителей с собственной архитектурой. Основные конкуренты не предлагают встроенных и быстрых оптимизаторов, что позволяет внедрить технологический задел для предлагаемого решения.
5.2. План маркетинга
5.2.1. План продаж
Маркетинговая стратегия проекта направлена на вывод на рынок подписочного решения. Данное решение позволит продемонстрировать ценность алгоритма и сформировать поток повторяющихся платежей [2].
Целевая аудитория – это малые и средние проектные команды (5-20 чел.), НИОКР-лаборатории, инжиниринговые стартапы в области микроэлектроники — организации, заинтересованные в снижении затрат на проектирование, но не готовые к крупным разовым инвестициям в ПО. Ссылка на таблицу 5.2?
Таблица 5.2 – «План продаж»
Показатели	I кв.	II кв.	III кв.	IV кв.	Всего
Подписка Starter 	 	 	 	 	 
Ожидаемый объем продаж, ед.	20	40	80	160	300
Цена с НДС, тыс. руб	30	30	30	30	120
Выручка с НДС, тыс. руб.	600	1200	2400	4800	9000
Нетто-выручка, тыс. руб.	480	960	1920	3840	7200
Сумма НДС, тыс. руб.	120	240	480	960	1800
 Подписка Professional	 	 	 	 	 
Ожидаемый объем продаж, ед.	1	3	8	19	31
Цена с НДС, тыс. руб	60	72	90	90	312
Выручка с НДС, тыс. руб.	60	216	720	1710	2706
Нетто-выручка, тыс. руб.	48	172,8	576	1368	2164,8
Сумма НДС, тыс. руб.	12	43,2	144	342	541,2
Итого	 	 	 	 	 
Выручка с НДС, тыс. руб.	660	1416	3120	6510	11706
Нетто-выручка, тыс. руб.	528	1132,8	2496	5208	9364,8
Сумма НДС, тыс. руб.	132	283,2	624	1302	2341,2

5.2.2. Товарная политика
Разрабатываемый продукт представляет собой интеллектуальную SaaS-решение (программное обеспечение как услуга). Продукт реализуется в виде месячных подписок с двумя тарифными планами, ориентированными на разные сегменты пользователей. Ссылка на табл?
 Таблица 5.3 – «Тарифный план»
Тариф	Стоимость (мес.)	Ключевые возможности
Starter	5 тыс. руб.	Базовый функционал, проекты до 10 тыс. LUT
Тариф	Стоимость (мес.)	Ключевые возможности
Professional	10-15 тыс. руб.	Полный доступ, приоритетная поддержка

Качество продукта подтверждается точностью предсказаний на уровне 70-90%. 
Техническое сопровождение обоснуется регулярными обновлениями и улучшением алгоритмов, первоначальной настройкой системы и приоритетной поддержкой для Professional клиентов. Гарантийные обязательства покрывают работоспособность основного функционала.
5.2.3. Ценовая политика
Метод ценообразования основан на подписочной модели с дифференциацией по функциональности. Базовый тариф Starter предлагается по фиксированной цене 30 тыс. руб./месяц, Professional - от 60 тыс. руб./месяц с прогрессивной шкалой в зависимости от увеличения объема ресурсов, поддерживаемых алгоритмом.
Для стимулирования долгосрочных отношений предусмотрена система лояльности, клиенты, продлевающие подписку на второй год, получают 10% скидку, на третий год - 15%. Особые условия предлагаются стратегическим партнерам и индустриальным заказчикам, готовым участвовать в развитии продукта.
Для образовательных учреждений действуют специальные расценки – академические лицензии доступны от 2 тыс. руб./месяц (скидка 40%).
5.2.4. Сбытовая политика и мероприятия
Основными каналами продукта являются прямые продажи через собственный отдел продаж и онлайн-продвижение через официальный сайт. Для взаимодействия с корпоративными клиентами создается специализированный отдел ключевых клиентов. Дополнительно планируется сотрудничество с партнерскими инжиниринговыми центрами.
Для продвижения продукта сделан упор на цифровые каналы, ориентированные на профессиональную аудиторию. Ссылка на табл???
Таблица 5.4 – «Расходы на рекламу»
Площадка	Бюджет (1-3 мес.)	Бюджет (после 3 мес.)	Формат рекламы
Яндекс.Директ	50 000 руб.	15 000 руб./мес.	Контекстная реклама по ключевым запросам
Habr	5000 руб.	5000 руб.	Написание статей о продукте и достижениях для инженеров
LinkedIn	20 000 руб.	20 000 руб./мес.	Нативная реклама в профильных сообществах

5.3. План производства
Так как продукт представляет собой программное обеспечение как услугу, основные производственные процессы сосредоточены на разработке и поддержке платформы. Производственный процесс организован по следующим направлениям
	Масштабирования объема решений
	Совершенствование алгоритмов машинного обучения
	Поддержка и обновление инфраструктуры
	Техническое сопровождение клиентов
Основные технологические операции и используемые ресурсы представлены в таблице ниже. Ссылка на табл???
 Таблица 5.5 – «Характеристика этапов производства»
Этап производства	Используемое оборудование	Используемые материалы	Количество специалистов	Трудоемкость (чел./ч)	Норма выработки (ед./ч)
Формирование датасетов	Сервер 	Проектные данные	1	160	0,00625
Разработка и улучшение алгоритмов	Сервер, ПК	IDE, датасеты	2	160	0,0125
Тестирование моделей	ПК	IDE	1	320	0,003125
Поддержка инфраструктуры	Серверы	IDE	2	640	0,003125
Техническая поддержка	ПК	База знаний	1	80	0,0125

Пример расчета нормы выработки в единицу времени (час) для этапа тестирования системы
	В_Н=(Ф_С*Р) / t_H=(1*1) / 80=0.0125 ед./ч.	(5.1)
Для запуска проекта потребуются инвестиционные вложения, которые необходимы для старта проекта. Основные статьи затрат представлены в таблицах далее. Ссылка на табл???
 Таблица 5.6 – «Характеристика необходимых зданий и сооружений»
№	Наименование помещения	Площадь (м²)	Способ получения	Стоимость (руб./мес)	Период получения
1	Офис разработки	60	Аренда	90 000	Месяц 1
2	Серверная	15	Аренда	45 000	Месяц 1
 	Всего	75	 	135 000	 

Ссылка на табл???
 Таблица 5.7 – «Перечень необходимого оборудования»
№	Наименование	Способ получения	Стоимость без НДС	Сумма НДС	Всего с НДС	Срок ввода
1	Cервер	Покупка	960 000	240 000	1 200 000	1 Месяц 
2	Рабочие станции (5 шт)	Покупка	360 000	90 000	450 000	1 Месяц 
3	Лицензии ПО	Покупка	20 000	5 000	25 000	1 Год
4	Офисное оборудование	Покупка	96 000	24 000	120 000	1 Месяц 
 	Итого	 	1 436 000	359 000	1 795 000	 

Информация о среднемесячной заработной плате ключевых специалистов в городе Санкт-Петербург, была получена на основании данных, представленных на сайте по трудоустройству [3]. Средний уровень заработной платы ML-инженера и инженера по работе с данными составляет 150 тысяч рублей, DevOps-инженера — 200 тысяч рублей. Заработная плата специалиста по технической поддержке и QA-инженера установлена на уровне 80 тысяч рублей в месяц.
Исходя из принятых нормативов в 21 рабочий день в месяц и продолжительность рабочего дня 8 часов, производится расчёт стоимости одного часа труда для каждой из рабочей позиций:
Стоимость одного часа работы ML-инженера:
	С_(ML-инженер)=(150000) / (21*8)=892.86 руб./ч.	(5.2)
Стоимость одного часа работы мобильного разработчика:
	С_(Data Engineer)=(150000) / (21*8)=892.86 руб./ч.	(5.3)
Стоимость одного часа работы backend разработчика:
	С_(DevOps-инженер)=(200000) / (21*8)=1190.48 руб./ч.	(5.4)
Стоимость одного часа работы специалиста поддержки:
	С_(Специалист поддержки)=(80000) / (21*8)=476.19 руб./ч.	(5.5)
Стоимость одного часа работы QA-инжинера:
	С_(QA-инженер)=(80000) / (21*8)=416.19 руб./ч.	(5.6)
Зная заработную плату исполнителей, можно вычислить основную и дополнительную заработную плату, а также расходы на социальные нужды.
	Формула для расчета основной заработной платы:
	З_(осн.зп.)=ΣТ*С,	(5.7)
где З_(осн.зп.) – расходы на основную заработную плату исполнителя в рублях, ΣТ – совокупное время на выполнение операций, С – дневная ставка исполнителя (руб./час).
Формула для расчета дополнительной заработной платы:
	З_(доп.зп.)=З_(осн.зп.)*(Н_доп  / 100),	(5.8)
где З_(доп.зп.)  – расходы на дополнительную заработную плату исполнителя в рублях, З_(осн.зп.) – расходы на основную заработную плату исполнителя в рублях, Н_доп  – норматив дополнительной заработной платы, в ВКР принимается равным 14%. 
Зная расходы на основную и заработную плату исполнителя, возможно рассчитать отчисления на социальные нужды (13):
	З_соц=〖(З〗_(осн.зп.)+ З_(доп.зп.))*(Н_соц  / 100),	(5.9)
где Н_соц – норматив отчислений страховых взносов на обязательное социальное, пенсионное и медицинское страхование, 30%. Ссылка на табл?
 Таблица 5.8 – «Расходы на заработную плату»
Исполнитель	Трудоемкость (чел./ч)	Основная зарплата (руб.)	Доп. зарплата (15%) (руб.)	Отчисления (руб.)
Data Engineer	160	142857,14	21428,57	64071,43
ML-инженер	160	142857,14	21428,57	64071,43
QA-инженер	320	152380,95	22857,14	68342,86
DevOps-инженер	640	761904,76	114285,71	341714,29
Специалист поддержки	80	38095,24	5714,29	17085,71
Итого	1360,00	1238095,24	185714,29	555285,71

В соответствии с Постановлением Правительства РФ от 01.01.2002 №1 (в редакции от 18.11.2022), определяющим «Классификацию основных средств, включаемых в амортизационные группы», серверное оборудование, персональные компьютеры и мобильные гаджеты классифицируются во 2-ую амортизационную группу, предусматривающую период полезного использования до 3 лет. Офисное оборудование, в свою очередь, отнесено к 4-ой группе, с максимальным сроком эксплуатации до 7 лет. Что касается лицензий на программное обеспечение, то их срок полезного использования установлен в размере 1 года.
	Н_А=(1 / Полезный срок использования) * 100%	(5.10)
Где ссылка на табл 5.9?
Таблица 5.9 – «Амортизационные отчисления» 
Группы основных производственных фондов (ОПФ)	Номер шага
	Год 0	Год 1	Год 2	Год 3
Техническое оборудование 	 
Начальная стоимость, тыс. руб.	1795,00	1795,00	1795,00	1795,00
Амортизационные отчисления, тыс. руб.	0,00	598,33	1196,67	1795,00
Остаточная стоимость, тыс. руб.	1795,00	1196,67	598,33	0,00
Офисное оборудование	 
Начальная стоимость, тыс. руб.	140,00	140,00	140,00	140,00
Амортизационные отчисления, тыс. руб.	0,00	20,00	40,00	60,00
Остаточная стоимость, тыс. руб.	140,00	120,00	100,00	80,00
Лицензии ПО 	 
Начальная стоимость, тыс. руб.	175,00	175,00	175,00	175,00
Амортизационные отчисления, тыс. руб.	0,00	175,00	0,00	0,00
Остаточная стоимость, тыс. руб.	175,00	0,00	175,00	175,00
Итого амортизационных отчислений, тыс. руб.	0,00	793,33	1236,67	1855,00

5.4. Финансовый план.
Горизонт планирования составляет 4 квартала (2 года). За данное количество времени рассчитывается, что проект окупится. Инфляция для данного проекта не учитывается в связи с краткосрочным горизонтом планирования. Все операции представлены в тыс. руб. Ссылка на табл.5.10?
 Таблица 5.10 – «Расчет дисконтированных денежных потоков»
Показатели	0 квартал	1 квартал	2 квартал	3 квартал	4 квартал
Операционная деятельность (ОД)	 	 	 	 	 
Выручка от реализации	0,00	660,00	1416,00	3120,00	6510,00
Производственные затраты	3243,10	1413,10	1413,10	1413,10	1413,10
Амортизация	0,00	0,00	618,33	618,33	927,50
Налогооблагаемая прибыль	0,00	0,00	0,00	0,00	4169,40
Налог на прибыль	0,00	0,00	0,00	0,00	833,88
Денежный поток от ОД	0,00	0,00	618,33	618,33	4263,02
Инвестиционная деятельность (ОД)	 
Инвестиции	-3500,00	-1500,00	-1500,00	-1500,00	0,00
Ликвидационная стоимость	1795,00	1795,00	1176,67	1176,67	867,50
Денежный поток (ДП) от ИД	-1705,00	295,00	-323,33	-323,33	867,50
ДП проекта	-1705,00	295,00	295,00	295,00	5130,52
ДП накопленным итогом (ЧД)	-1705,00	-1410,00	-1115,00	-820,00	4310,52
Коэффициент дисконтирования (12%)	1,00	0,89	0,80	0,71	0,64
Дисконтированный ДП (ДДП)	-1705,00	263,39	235,17	209,98	3260,54
ДДП накопленным итогом (ЧДД)	-1705,00	-1258,93	-888,87	-583,66	2739,42

На основании таблицы 5.10 произведён анализ эффективности инвестиционного проекта. Чистая текущая стоимость проекта рассчитывается по формуле
NPV=∑_(t=1)^T▒CIF_t/〖(1+R)〗^t -∑_(t=1)^T▒〖COF〗_t/(1+R)^t = 2739,42 			(5.11)
где CIFₜ — поступления денежных средств, связанных с реализацией проекта в интервале t (притоки денежных средств); COFₜ — платежи (оттоки денежных средств), связанные с реализацией проекта в интервале t; R — ставка дисконтирования, применяемая для оценки проекта; T — длительность инвестиционного периода, выраженная в количестве интервалов (процентных периодов), по окончании которых начисляются проценты.
В качестве дополнительного показателя эффективности определена внутренняя норма доходности проекта (IRR), которая рассчитывается на основе уравнивания дисконтированных притоков и оттоков
∑_(t=1)^T▒CIF_t/〖(1+IRR)〗^t = ∑_(t=1)^T▒COF_t/〖(1+IRR)〗^t 			 (5.12)
где IRR – искомая ставка внутренней рентабельности проекта.
По результатам расчётов IRR составляет 0,42 (или 42 %)
Из всего вышесказанного можно заявить, что проект характеризуется положительным значением чистой текущей стоимости, высокой внутренней нормой доходности и коротким сроком окупаемости. Всё это свидетельствует о его финансовой целесообразности и эффективности реализации. 
ЗАКЛЮЧЕНИЕ
 

СПИСОК ИСПОЛЬЗОВАННЫХ ИСТОЧНИКОВ
[1] Микроэлектроника (мировой рынок) [Электронный ресурс]. URL: https://www.tadviser.ru/index.php/Статья:Микроэлектроника_(мировой_рынок) (дата обращения: 10.05.25).
[2] Software as a Service: Strategic Backgrounder. – Washington, DC: Software & Information Industry Association, 2001. – 18 с. 
[3] hh.ru [Электронный ресурс]. URL: https://hh.ru (дата обращения: 10.05.25).

